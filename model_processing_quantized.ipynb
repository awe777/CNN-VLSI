{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kuantisasi dan verifikasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from tensorflow.keras.models import load_model,model_from_json\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from tensorflow.keras.initializers import glorot_uniform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model dari file h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model reconstruction from JSON file\n",
    "with open('model.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('CNN_Model.h5')\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "with h5py.File('CNN_Model.h5','r') as hdf:\n",
    "    layer_1_data = np.array(hdf.get('layer_1/layer_1/kernel:0'))\n",
    "    layer_2_data = np.array(hdf.get('layer_2/layer_2/kernel:0'))\n",
    "    layer_3_data = np.array(hdf.get('layer_3/layer_3/kernel:0'))\n",
    "    layer_out_data = np.array(hdf.get('layer_Output/layer_Output/kernel:0'))\n",
    "    \n",
    "#load dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fungsi - fungsi CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(mat,ksize,method='max',pad=False):\n",
    "    m, n = mat.shape[1:]\n",
    "    ky,kx=ksize\n",
    "\n",
    "    _ceil=lambda x,y: int(numpy.ceil(x/float(y)))\n",
    "\n",
    "    if pad:\n",
    "        ny=_ceil(m,ky)\n",
    "        nx=_ceil(n,kx)\n",
    "        size=(mat.shape[0],ny*ky, nx*kx)\n",
    "        mat_pad=numpy.full(size,numpy.nan)\n",
    "        mat_pad[:m,:n,...]=mat\n",
    "    else:\n",
    "        ny=m//ky\n",
    "        nx=n//kx\n",
    "        mat_pad=mat[..., :ny*ky, :nx*kx]\n",
    "\n",
    "    new_shape=(mat.shape[0],ny,ky,nx,kx)\n",
    "\n",
    "    if method=='max':\n",
    "        result=np.nanmax(mat_pad.reshape(new_shape),axis=(2,4))\n",
    "    else:\n",
    "        result=np.nanmean(mat_pad.reshape(new_shape),axis=(2,4))\n",
    "\n",
    "    return result\n",
    "\n",
    "def channel_im2col(A):\n",
    "    B = [3,3]\n",
    "    skip=[1,1]\n",
    "    # Parameters \n",
    "    D,M,N = A.shape\n",
    "    col_extent = N - B[1] + 1\n",
    "    row_extent = M - B[0] + 1\n",
    "\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(B[0])[:,None]*N + np.arange(B[1])\n",
    "\n",
    "    # Generate Depth indeces\n",
    "    didx=M*N*np.arange(D)\n",
    "    start_idx=(didx[:,None]+start_idx.ravel()).reshape((-1,B[0],B[1]))\n",
    "\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[:,None]*N + np.arange(col_extent)\n",
    "\n",
    "    # Get all actual indices & index into input array for final output\n",
    "    out = np.take (A,start_idx.ravel()[:,None] + offset_idx[::skip[0],::skip[1]].ravel())\n",
    "    return out\n",
    "\n",
    "\n",
    "def conv2d(filt,act):\n",
    "    '''\n",
    "    conv 2d function\n",
    "    act = widht, height,depth (w1,h1,d1)\n",
    "    default = 3,3 filter, stride = 1, p = 0\n",
    "    im2col reshape\n",
    "    \n",
    "    W2=(W1−F+2P)/S+1\n",
    "    H2=(H1−F+2P)/S+1\n",
    "    D2=K\n",
    "    '''\n",
    "    P = 0\n",
    "    S = 1\n",
    "    D1,H1,W1 = act.shape\n",
    "    n_f,d_f,h_f,w_f = filt.shape\n",
    "    window_size = d_f*h_f*w_f\n",
    "    filter_reshape = n_f,window_size\n",
    "    W2=(W1-h_f +2*P)//S+1\n",
    "    H2=(H1- h_f+2*P)//S+1\n",
    "    D2= n_f\n",
    "    m = channel_im2col(act)\n",
    "    filt = filt.reshape(filter_reshape)\n",
    "    res = np.matmul(filt, m)\n",
    "    res = res.reshape(D2,W2,H2)\n",
    "    return res\n",
    "\n",
    "def softmax(X):\n",
    "    expo = np.exp(X)\n",
    "    expo_sum = np.sum(np.exp(X))\n",
    "    return expo/expo_sum\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def prequant(data,bits):\n",
    "    MaxValue = np.amax(data)\n",
    "    MinValue = np.amin(data)\n",
    "    Range_Real = MaxValue - MinValue\n",
    "    if(Range_Real==0):\n",
    "        Range_Real = 1\n",
    "    Scale = (Range_Real/(pow(2,bits)-1))\n",
    "    return Scale, Range_Real\n",
    "\n",
    "def quantize(Val, Range_Real, Scale, zero_point):  \n",
    "    temp = round(Val * (1/Scale) + zero_point)\n",
    "    return temp\n",
    "quantizefunc = np.vectorize(quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_data = layer_1_data.transpose(3,2,0,1)\n",
    "layer_2_data= layer_2_data.transpose(3,2,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Terkuantisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model2(img):\n",
    "    Scale_w1,Range_Real_w1 = prequant(layer_1_data,7)\n",
    "    Scale_d1,Range_Real_d1 = prequant(img.reshape(1,28,28),7)\n",
    "    quantized_w1 = quantizefunc(layer_1_data,Range_Real_w1,Scale_w1,0)\n",
    "    quantized_d1 = quantizefunc(img.reshape(1,28,28),Range_Real_d1,Scale_d1,0)\n",
    "    \n",
    "    layer_1_out = relu(conv2d(quantized_w1,quantized_d1))\n",
    "    layer_1_out = Scale_d1*Scale_w1*pool2d(layer_1_out, (2, 2))\n",
    "    \n",
    "    Scale_w2,Range_Real_w2 = prequant(layer_2_data,7)\n",
    "    Scale_d2,Range_Real_d2 = prequant(layer_1_out,7)\n",
    "    quantized_w2 = quantizefunc(layer_2_data,Range_Real_w2,Scale_w2,0)\n",
    "    quantized_d2 = quantizefunc(layer_1_out,Range_Real_d2,Scale_d2,0)\n",
    "    \n",
    "    layer_2_out = relu(conv2d(quantized_w2,quantized_d2))\n",
    "    layer_2_out = Scale_d2*Scale_w2*pool2d(layer_2_out, (2, 2))\n",
    "    layer_2_out = layer_2_out.transpose(1,2,0) ### ubah format data dari CHW ke WHC\n",
    "    layer_2_out = layer_2_out.reshape(16*5*5)\n",
    "    \n",
    "    Scale_w3,Range_Real_w3 = prequant(layer_3_data,7)\n",
    "    Scale_d3,Range_Real_d3 = prequant(layer_2_out,7)\n",
    "    quantized_w3 = quantizefunc(layer_3_data,Range_Real_w3,Scale_w3,0)\n",
    "    quantized_d3 = quantizefunc(layer_2_out,Range_Real_d3,Scale_d3,0)\n",
    "    \n",
    "    layer_3_out = np.matmul(quantized_d3,quantized_w3)\n",
    "    layer_3_out = Scale_d3*Scale_w3*relu(layer_3_out)\n",
    "    \n",
    "    Scale_w4,Range_Real_w4 = prequant(layer_out_data,7)\n",
    "    Scale_d4,Range_Real_d4 = prequant(layer_3_out,7)\n",
    "    quantized_w4 = quantizefunc(layer_out_data,Range_Real_w4,Scale_w4,0)\n",
    "    quantized_d4 = quantizefunc(layer_3_out,Range_Real_d4,Scale_d4,0)\n",
    "    \n",
    "    layer_last_out = Scale_d4*Scale_w4*np.matmul(quantized_d4,quantized_w4)\n",
    "    layer_last_out = softmax(layer_last_out)\n",
    "    return layer_last_out,\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifikasi Kuantisasi\n",
    "\n",
    "Di code ini model yang terkuantisasi dibandingkan akurasinya dengan model yang tidak terkuantisasi berdasarkan akurasi dengan dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_num_uq = 0\n",
    "val_num_q = 0\n",
    "correct_q = np.empty(10000)\n",
    "correct_uq = np.empty(10000)\n",
    "val_num_q_2 = 0\n",
    "correct_q_2 = np.empty(10000)\n",
    "\n",
    "for i in range(10000):\n",
    "    indata = x_test[i] /255\n",
    "    label = y_test[i]\n",
    "    \n",
    "    #data = indata.ravel()\n",
    "    \n",
    "    keras_res = model.predict_classes(indata.reshape(1,28,28,1))\n",
    "    res = Model2(indata.reshape(1,28,28))\n",
    "    \n",
    "    if(label==keras_res[0]):        \n",
    "        correct_q[val_num_q] = i\n",
    "        val_num_q += 1\n",
    "\n",
    "    if(label==np.argmax(res)):\n",
    "        correct_uq[val_num_uq] = i\n",
    "        val_num_uq += 1\n",
    "        \n",
    "print('val_num_keras_model :', val_num_q)\n",
    "print('val_num_quantized_model :', val_num_uq)\n",
    "\n",
    "print('val val_num_keras_model :', (val_num_q/10000)*100,'%')\n",
    "print('val quantized  :', (val_num_uq/10000)*100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melihat / mengambil weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_data.shape #16 filter, 1 filter per channel, size 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_data[0][0] #weight filter pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
