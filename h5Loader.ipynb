{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow.python.keras as keras\n",
    "from keras.layers import Input, Layer, LeakyReLU, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Add, Lambda\n",
    "from keras.models import Model, load_model, model_from_json, clone_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "import os\n",
    "K.clear_session()\n",
    "'''\n",
    "# file h5 dibuat dengan convert.py dari https://github.com/awe777/keras-yolo3\n",
    "model0 = load_model(\"yolov3_tiny.h5\")   # dibuat dari kode sebelum fork\n",
    "model1 = load_model(\"yolov3_tiny_.h5\")  # dibuat dari kode setelah diubah di fork pribadi\n",
    "print(model0.to_json() == model1.to_json())\n",
    "print(model.summary())\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom layer classes successfully defined\n"
     ]
    }
   ],
   "source": [
    "def roundingAlgo(x):\n",
    "    return K.round(x)\n",
    "\n",
    "class RoundQinf_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundQinf_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundQinf_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return roundingAlgo(X * 16) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundQinf_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ7_12(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ7_12, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ7_12, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 4096) + 524288) % 1048576) - 524288) / 4096.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ7_12, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundQx_8minusx(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundQx_8minusx, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundQx_8minusx, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        pow_2_to_expplus8 = 2 ** (8 + (K.log(K.abs(X)) / math.log(2)) // 1)\n",
    "        return roundingAlgo(X * pow_2_to_expplus8) / (pow_2_to_expplus8 * 1.0)\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundQx_8minusx, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ3_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ3_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ3_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 16) + 128) % 256) - 128) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ3_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ7_0(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ7_0, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ7_0, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return ((roundingAlgo(X) + 128) % 256) - 128\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ7_0, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ7_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ7_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ7_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 16) + 2048) % 4096) - 2048) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ7_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ11_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ11_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ11_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 16) + 32768) % 65536) - 32768) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ11_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class Identity(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Identity, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def call(self, X):\n",
    "        return X\n",
    "    def get_config(self):\n",
    "        base_config = super(Identity, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class IdentityFinalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(IdentityFinalLayer, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def call(self, X):\n",
    "        return X\n",
    "    def get_config(self):\n",
    "        base_config = super(IdentityFinalLayer, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "\n",
    "print(\"Custom layer classes successfully defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customObjects = {\n",
    "    'Identity': Identity,\n",
    "    'RoundQinf_4': RoundQinf_4,\n",
    "    'RoundOverflowQ3_4': RoundOverflowQ3_4,\n",
    "    'RoundQx_8minusx': RoundQx_8minusx,\n",
    "    'RoundOverflowQ7_12': RoundOverflowQ7_12,\n",
    "    'RoundOverflowQ7_0': RoundOverflowQ7_0,\n",
    "    'RoundOverflowQ7_4': RoundOverflowQ7_4,\n",
    "    'RoundOverflowQ11_4': RoundOverflowQ11_4,\n",
    "    'IdentityFinalLayer': IdentityFinalLayer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_dir, input_shape):\n",
    "    # from PIL import Image\n",
    "    # import numpy as np\n",
    "    image_data = 0\n",
    "    h, w = input_shape\n",
    "    try:\n",
    "        image = Image.open(image_dir)\n",
    "        iw, ih = image.size\n",
    "        scale = min(w/iw, h/ih)\n",
    "        nw = int(iw*scale)\n",
    "        nh = int(ih*scale)\n",
    "        dx = (w-nw)//2\n",
    "        dy = (h-nh)//2\n",
    "        image = image.resize((nw,nh), Image.BICUBIC)\n",
    "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        image_data = np.array(new_image)/255.\n",
    "        return image_data, image.size[::-1]\n",
    "    except:\n",
    "        return None, None\n",
    "# TO DO: implement yolo_head in order to get human-understandable result from model output\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "tinyYolo_anchors = get_anchors(\"../CNN-VLSI/tiny_yolo_anchors.txt\")\n",
    "\n",
    "def yolo_head_nonLoss(feats, anchors, num_classes, input_shape):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]), [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]), [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, 'float32') # K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    # edited by instructions in https://stackoverflow.com/questions/57558476/training-a-keras-model-yields-multiple-optimizer-errors\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[...,::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[...,::-1], K.dtype(feats))\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n",
    "    offset = (input_shape-new_shape)/2./input_shape\n",
    "    scale = input_shape/new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes =  K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head_nonLoss(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [0,1,2]] # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "#     print(\"Box tensor:\", boxes.shape)\n",
    "#     print(K.get_value(boxes))\n",
    "#     print(\"Box score tensor:\", box_scores.shape)\n",
    "#     print(K.get_value(box_scores))\n",
    "#     print(\"class index of maximum prob value:\", K.argmax(box_scores, axis=1).shape)\n",
    "#     print(K.get_value(K.argmax(box_scores, axis=1)))\n",
    "#     print(\"maximum prob value: \",K.max(box_scores, axis=1).shape)\n",
    "#     print(K.get_value(K.max(box_scores, axis=1)))\n",
    "#     print(\"maximum prob mask: \", (K.max(box_scores, axis=1) >= score_threshold).shape)\n",
    "#     print(K.get_value(K.max(box_scores, axis=1) >= score_threshold)) # 0.027))\n",
    "#     print()\n",
    "#     print(\"index of highest maximum prob value:\",K.get_value(K.argmax(K.max(box_scores, axis=1))))\n",
    "#     print(\"highest maximum prob value:\", K.get_value(K.max(box_scores)))\n",
    "#     print(\"class index:\", K.get_value(K.argmax(box_scores, axis=1))[K.get_value(K.argmax(K.max(box_scores, axis=1)))])\n",
    "#     print(\"raw coordinates:\", K.get_value(boxes)[K.get_value(K.argmax(K.max(box_scores, axis=1)))])\n",
    "#     print(\"class index:\", K.get_value(box_scores)[K.get_value(K.argmax(K.max(box_scores, axis=1)))])\n",
    "    boxes_dummy = []\n",
    "    scores_dummy = []\n",
    "    classes_dummy = []\n",
    "    scoremask_dummy = K.get_value(K.max(box_scores, axis=1) >= score_threshold)\n",
    "    for i in range(scoremask_dummy.size):\n",
    "        if scoremask_dummy[i]:\n",
    "            boxes_dummy.append(K.get_value(boxes)[i].tolist())\n",
    "            scores_dummy.append(K.get_value(K.max(box_scores, axis=1))[i].tolist())\n",
    "            classes_dummy.append(K.get_value(K.argmax(box_scores, axis=1))[i].tolist())\n",
    "    ''' # toggle to utilize either hacky solution with working classes output or IOU result-cutting solution with non-working classes output\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "    '''\n",
    "    boxes_ = boxes_dummy\n",
    "    scores_ = scores_dummy\n",
    "    classes_ = classes_dummy\n",
    "    '''\n",
    "    # '''\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceProcessor(jsonModel, fileOutput, imgFolderSource, countNotification=1000, boundBox=False):\n",
    "    loadedModel = None\n",
    "    try:\n",
    "        with open(jsonModel, \"r\") as jsonFile:\n",
    "            jsonRead = jsonFile.read()\n",
    "            # NOTE: this *will* take time, may cause OOM warnings\n",
    "            loadedModel = model_from_json(jsonRead, custom_objects=customObjects)\n",
    "            copyLoadModel = clone_model(loadedModel)\n",
    "            loadedModel = copyLoadModel\n",
    "            del copyLoadModel\n",
    "        print(\"Model loading successful\")\n",
    "        loadedModel.load_weights(\"./saved_models/model_0_trainModel.h5\", by_name=False)    \n",
    "        print(\"Model weight loading attempt successful\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load model/weight data:\", e)\n",
    "    if loadedModel is not None:\n",
    "        count = 0\n",
    "        outputFile = open(fileOutput, \"a\")\n",
    "        for folderPath in imgFolderSource:\n",
    "            if count is not 0:\n",
    "                print(\"Switching to another folder\")\n",
    "            with os.scandir(folderPath) as folder:\n",
    "                for entry in folder:\n",
    "                    if entry.name.endswith(\".jpg\") and entry.is_file():\n",
    "                        fileName = str(entry.path)\n",
    "                        prepped_image = prepare_image(entry.path, (448,448))\n",
    "                        processedOutput = yolo_eval(\n",
    "                            yolo_outputs = loadedModel.predict(\n",
    "                                x = np.expand_dims(prepped_image[0], axis=0),\n",
    "                                verbose=0,\n",
    "                                steps=1\n",
    "                            ),\n",
    "                            anchors=tinyYolo_anchors,\n",
    "                            num_classes=80,\n",
    "                            image_shape=prepped_image[1], # image shape IS NOT model input shape, which is (448, 448)\n",
    "                            max_boxes=20,\n",
    "                            score_threshold=0.7,\n",
    "                            iou_threshold=0.5\n",
    "                        )\n",
    "                        ''' \n",
    "                            processedOutput = [\n",
    "                                [each inference's bounding box with format [top, left, bottom, right] in a list],\n",
    "                                [each inference's score in a list],\n",
    "                                [each inference's class in a list],\n",
    "                            ] \n",
    "                            # len(processedOutput) is always 3\n",
    "                            # len(processedOutput[0]) == len(processedOutput[1]) == len(processedOutput[2]) == number of inference\n",
    "                            # len(processedOutput[0][k]) is always 4 for valid k\n",
    "                        '''\n",
    "                        if boundBox:\n",
    "                            for eachBBox in processedOutput[0]:\n",
    "                                eachBBox[0] = max(0.0, eachBBox[0]) # bound top to 0\n",
    "                                eachBBox[1] = max(0.0, eachBBox[1]) # bound left to 0\n",
    "                                eachBBox[2] = min(prepped_image[1][0] / 1.0, eachBBox[2]) # bound bottom to image height\n",
    "                                eachBBox[3] = min(prepped_image[1][1] / 1.0, eachBBox[3]) # bound right to image width\n",
    "                        outputFile.write(json.dumps([fileName, processedOutput]) + \"\\n\")\n",
    "                        count = count + 1\n",
    "                        if count % countNotification == 0:\n",
    "                            print(\"# of image processed:\", count)        \n",
    "        outputFile.close()\n",
    "        print(count, \"images successfully processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inference model: ./saved_models/model_0_inferenceModel.json\n",
      "Model loading successful\n",
      "Model weight loading attempt successful\n",
      "# of image processed: 1000\n",
      "# of image processed: 2000\n",
      "# of image processed: 3000\n",
      "# of image processed: 4000\n",
      "# of image processed: 5000\n",
      "# of image processed: 6000\n",
      "# of image processed: 7000\n",
      "# of image processed: 8000\n",
      "# of image processed: 9000\n",
      "# of image processed: 10000\n",
      "# of image processed: 11000\n",
      "# of image processed: 12000\n",
      "# of image processed: 13000\n",
      "# of image processed: 14000\n",
      "# of image processed: 15000\n",
      "# of image processed: 16000\n",
      "# of image processed: 17000\n",
      "# of image processed: 18000\n",
      "# of image processed: 19000\n",
      "# of image processed: 20000\n",
      "# of image processed: 21000\n",
      "# of image processed: 22000\n",
      "# of image processed: 23000\n",
      "# of image processed: 24000\n",
      "# of image processed: 25000\n",
      "# of image processed: 26000\n",
      "# of image processed: 27000\n",
      "# of image processed: 28000\n",
      "# of image processed: 29000\n",
      "# of image processed: 30000\n",
      "# of image processed: 31000\n",
      "# of image processed: 32000\n",
      "# of image processed: 33000\n",
      "# of image processed: 34000\n",
      "# of image processed: 35000\n",
      "# of image processed: 36000\n",
      "# of image processed: 37000\n",
      "# of image processed: 38000\n",
      "# of image processed: 39000\n",
      "# of image processed: 40000\n",
      "# of image processed: 41000\n",
      "# of image processed: 42000\n",
      "# of image processed: 43000\n",
      "# of image processed: 44000\n",
      "# of image processed: 45000\n",
      "# of image processed: 46000\n",
      "# of image processed: 47000\n",
      "# of image processed: 48000\n",
      "# of image processed: 49000\n",
      "# of image processed: 50000\n",
      "# of image processed: 51000\n",
      "# of image processed: 52000\n",
      "# of image processed: 53000\n",
      "# of image processed: 54000\n",
      "# of image processed: 55000\n",
      "# of image processed: 56000\n",
      "# of image processed: 57000\n",
      "# of image processed: 58000\n",
      "# of image processed: 59000\n",
      "# of image processed: 60000\n",
      "# of image processed: 61000\n",
      "# of image processed: 62000\n",
      "# of image processed: 63000\n",
      "# of image processed: 64000\n",
      "# of image processed: 65000\n",
      "# of image processed: 66000\n",
      "# of image processed: 67000\n",
      "# of image processed: 68000\n",
      "# of image processed: 69000\n",
      "# of image processed: 70000\n",
      "# of image processed: 71000\n",
      "# of image processed: 72000\n",
      "# of image processed: 73000\n",
      "# of image processed: 74000\n",
      "# of image processed: 75000\n",
      "# of image processed: 76000\n",
      "# of image processed: 77000\n",
      "# of image processed: 78000\n",
      "# of image processed: 79000\n",
      "# of image processed: 80000\n",
      "# of image processed: 81000\n",
      "# of image processed: 82000\n",
      "# of image processed: 83000\n",
      "# of image processed: 84000\n",
      "# of image processed: 85000\n",
      "# of image processed: 86000\n",
      "# of image processed: 87000\n",
      "# of image processed: 88000\n",
      "# of image processed: 89000\n",
      "# of image processed: 90000\n",
      "# of image processed: 91000\n",
      "# of image processed: 92000\n",
      "# of image processed: 93000\n",
      "# of image processed: 94000\n",
      "# of image processed: 95000\n",
      "# of image processed: 96000\n",
      "# of image processed: 97000\n",
      "# of image processed: 98000\n",
      "# of image processed: 99000\n",
      "# of image processed: 100000\n",
      "# of image processed: 101000\n",
      "# of image processed: 102000\n",
      "# of image processed: 103000\n",
      "# of image processed: 104000\n",
      "# of image processed: 105000\n",
      "# of image processed: 106000\n",
      "# of image processed: 107000\n",
      "# of image processed: 108000\n",
      "# of image processed: 109000\n",
      "# of image processed: 110000\n",
      "# of image processed: 111000\n",
      "# of image processed: 112000\n",
      "# of image processed: 113000\n",
      "# of image processed: 114000\n",
      "# of image processed: 115000\n",
      "# of image processed: 116000\n",
      "# of image processed: 117000\n",
      "# of image processed: 118000\n",
      "Switching to another folder\n",
      "# of image processed: 119000\n",
      "# of image processed: 120000\n",
      "# of image processed: 121000\n",
      "# of image processed: 122000\n",
      "# of image processed: 123000\n",
      "# of image processed: 124000\n",
      "# of image processed: 125000\n",
      "# of image processed: 126000\n",
      "# of image processed: 127000\n",
      "# of image processed: 128000\n",
      "# of image processed: 129000\n",
      "# of image processed: 130000\n",
      "# of image processed: 131000\n",
      "# of image processed: 132000\n",
      "# of image processed: 133000\n",
      "# of image processed: 134000\n",
      "# of image processed: 135000\n",
      "# of image processed: 136000\n",
      "# of image processed: 137000\n",
      "# of image processed: 138000\n",
      "# of image processed: 139000\n",
      "# of image processed: 140000\n",
      "# of image processed: 141000\n",
      "# of image processed: 142000\n",
      "# of image processed: 143000\n",
      "# of image processed: 144000\n",
      "# of image processed: 145000\n",
      "# of image processed: 146000\n",
      "# of image processed: 147000\n",
      "# of image processed: 148000\n",
      "# of image processed: 149000\n",
      "# of image processed: 150000\n",
      "# of image processed: 151000\n",
      "# of image processed: 152000\n",
      "# of image processed: 153000\n",
      "# of image processed: 154000\n",
      "# of image processed: 155000\n",
      "# of image processed: 156000\n",
      "# of image processed: 157000\n",
      "# of image processed: 158000\n",
      "158957 images successfully processed\n",
      "Using inference model: ./saved_models/model_1_inferenceModel.json\n",
      "Model loading successful\n",
      "Model weight loading attempt successful\n",
      "# of image processed: 1000\n",
      "# of image processed: 2000\n",
      "# of image processed: 3000\n",
      "# of image processed: 4000\n",
      "# of image processed: 5000\n",
      "# of image processed: 6000\n",
      "# of image processed: 7000\n",
      "# of image processed: 8000\n",
      "# of image processed: 9000\n",
      "# of image processed: 10000\n",
      "# of image processed: 11000\n",
      "# of image processed: 12000\n",
      "# of image processed: 13000\n",
      "# of image processed: 14000\n",
      "# of image processed: 15000\n",
      "# of image processed: 16000\n",
      "# of image processed: 17000\n",
      "# of image processed: 18000\n",
      "# of image processed: 19000\n",
      "# of image processed: 20000\n",
      "# of image processed: 21000\n",
      "# of image processed: 22000\n",
      "# of image processed: 23000\n",
      "# of image processed: 24000\n",
      "# of image processed: 25000\n",
      "# of image processed: 26000\n",
      "# of image processed: 27000\n",
      "# of image processed: 28000\n",
      "# of image processed: 29000\n",
      "# of image processed: 30000\n",
      "# of image processed: 31000\n",
      "# of image processed: 32000\n",
      "# of image processed: 33000\n",
      "# of image processed: 34000\n",
      "# of image processed: 35000\n",
      "# of image processed: 36000\n",
      "# of image processed: 37000\n",
      "# of image processed: 38000\n",
      "# of image processed: 39000\n",
      "# of image processed: 40000\n",
      "# of image processed: 41000\n",
      "# of image processed: 42000\n",
      "# of image processed: 43000\n",
      "# of image processed: 44000\n",
      "# of image processed: 45000\n",
      "# of image processed: 46000\n",
      "# of image processed: 47000\n",
      "# of image processed: 48000\n",
      "# of image processed: 49000\n",
      "# of image processed: 50000\n",
      "# of image processed: 51000\n",
      "# of image processed: 52000\n",
      "# of image processed: 53000\n",
      "# of image processed: 54000\n",
      "# of image processed: 55000\n",
      "# of image processed: 56000\n",
      "# of image processed: 57000\n",
      "# of image processed: 58000\n",
      "# of image processed: 59000\n",
      "# of image processed: 60000\n",
      "# of image processed: 61000\n",
      "# of image processed: 62000\n",
      "# of image processed: 63000\n",
      "# of image processed: 64000\n",
      "# of image processed: 65000\n",
      "# of image processed: 66000\n",
      "# of image processed: 67000\n",
      "# of image processed: 68000\n",
      "# of image processed: 69000\n",
      "# of image processed: 70000\n",
      "# of image processed: 71000\n",
      "# of image processed: 72000\n",
      "# of image processed: 73000\n",
      "# of image processed: 74000\n",
      "# of image processed: 75000\n",
      "# of image processed: 76000\n",
      "# of image processed: 77000\n",
      "# of image processed: 78000\n",
      "# of image processed: 79000\n",
      "# of image processed: 80000\n",
      "# of image processed: 81000\n",
      "# of image processed: 82000\n",
      "# of image processed: 83000\n",
      "# of image processed: 84000\n",
      "# of image processed: 85000\n",
      "# of image processed: 86000\n",
      "# of image processed: 87000\n",
      "# of image processed: 88000\n",
      "# of image processed: 89000\n",
      "# of image processed: 90000\n",
      "# of image processed: 91000\n",
      "# of image processed: 92000\n",
      "# of image processed: 93000\n",
      "# of image processed: 94000\n",
      "# of image processed: 95000\n",
      "# of image processed: 96000\n",
      "# of image processed: 97000\n",
      "# of image processed: 98000\n",
      "# of image processed: 99000\n",
      "# of image processed: 100000\n",
      "# of image processed: 101000\n",
      "# of image processed: 102000\n",
      "# of image processed: 103000\n",
      "# of image processed: 104000\n",
      "# of image processed: 105000\n",
      "# of image processed: 106000\n",
      "# of image processed: 107000\n",
      "# of image processed: 108000\n",
      "# of image processed: 109000\n",
      "# of image processed: 110000\n",
      "# of image processed: 111000\n",
      "# of image processed: 112000\n",
      "# of image processed: 113000\n",
      "# of image processed: 114000\n",
      "# of image processed: 115000\n",
      "# of image processed: 116000\n",
      "# of image processed: 117000\n",
      "# of image processed: 118000\n",
      "Switching to another folder\n",
      "# of image processed: 119000\n",
      "# of image processed: 120000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of image processed: 121000\n",
      "# of image processed: 122000\n",
      "# of image processed: 123000\n",
      "# of image processed: 124000\n",
      "# of image processed: 125000\n",
      "# of image processed: 126000\n",
      "# of image processed: 127000\n",
      "# of image processed: 128000\n",
      "# of image processed: 129000\n",
      "# of image processed: 130000\n",
      "# of image processed: 131000\n",
      "# of image processed: 132000\n",
      "# of image processed: 133000\n",
      "# of image processed: 134000\n",
      "# of image processed: 135000\n",
      "# of image processed: 136000\n",
      "# of image processed: 137000\n",
      "# of image processed: 138000\n",
      "# of image processed: 139000\n",
      "# of image processed: 140000\n",
      "# of image processed: 141000\n",
      "# of image processed: 142000\n",
      "# of image processed: 143000\n",
      "# of image processed: 144000\n",
      "# of image processed: 145000\n",
      "# of image processed: 146000\n",
      "# of image processed: 147000\n",
      "# of image processed: 148000\n",
      "# of image processed: 149000\n",
      "# of image processed: 150000\n",
      "# of image processed: 151000\n",
      "# of image processed: 152000\n",
      "# of image processed: 153000\n",
      "# of image processed: 154000\n",
      "# of image processed: 155000\n",
      "# of image processed: 156000\n",
      "# of image processed: 157000\n",
      "# of image processed: 158000\n",
      "158957 images successfully processed\n",
      "Using inference model: ./saved_models/model_5_inferenceModel.json\n",
      "Model loading successful\n",
      "Model weight loading attempt successful\n",
      "# of image processed: 1000\n",
      "# of image processed: 2000\n",
      "# of image processed: 3000\n",
      "# of image processed: 4000\n",
      "# of image processed: 5000\n",
      "# of image processed: 6000\n",
      "# of image processed: 7000\n",
      "# of image processed: 8000\n",
      "# of image processed: 9000\n",
      "# of image processed: 10000\n",
      "# of image processed: 11000\n",
      "# of image processed: 12000\n",
      "# of image processed: 13000\n",
      "# of image processed: 14000\n",
      "# of image processed: 15000\n",
      "# of image processed: 16000\n",
      "# of image processed: 17000\n",
      "# of image processed: 18000\n",
      "# of image processed: 19000\n",
      "# of image processed: 20000\n",
      "# of image processed: 21000\n",
      "# of image processed: 22000\n",
      "# of image processed: 23000\n",
      "# of image processed: 24000\n",
      "# of image processed: 25000\n",
      "# of image processed: 26000\n",
      "# of image processed: 27000\n",
      "# of image processed: 28000\n",
      "# of image processed: 29000\n",
      "# of image processed: 30000\n",
      "# of image processed: 31000\n",
      "# of image processed: 32000\n",
      "# of image processed: 33000\n",
      "# of image processed: 34000\n",
      "# of image processed: 35000\n",
      "# of image processed: 36000\n",
      "# of image processed: 37000\n",
      "# of image processed: 38000\n",
      "# of image processed: 39000\n",
      "# of image processed: 40000\n",
      "# of image processed: 41000\n",
      "# of image processed: 42000\n",
      "# of image processed: 43000\n",
      "# of image processed: 44000\n",
      "# of image processed: 45000\n",
      "# of image processed: 46000\n",
      "# of image processed: 47000\n",
      "# of image processed: 48000\n",
      "# of image processed: 49000\n",
      "# of image processed: 50000\n",
      "# of image processed: 51000\n",
      "# of image processed: 52000\n",
      "# of image processed: 53000\n",
      "# of image processed: 54000\n",
      "# of image processed: 55000\n",
      "# of image processed: 56000\n",
      "# of image processed: 57000\n",
      "# of image processed: 58000\n",
      "# of image processed: 59000\n",
      "# of image processed: 60000\n",
      "# of image processed: 61000\n",
      "# of image processed: 62000\n",
      "# of image processed: 63000\n",
      "# of image processed: 64000\n",
      "# of image processed: 65000\n",
      "# of image processed: 66000\n",
      "# of image processed: 67000\n",
      "# of image processed: 68000\n",
      "# of image processed: 69000\n",
      "# of image processed: 70000\n",
      "# of image processed: 71000\n",
      "# of image processed: 72000\n",
      "# of image processed: 73000\n",
      "# of image processed: 74000\n",
      "# of image processed: 75000\n",
      "# of image processed: 76000\n",
      "# of image processed: 77000\n",
      "# of image processed: 78000\n",
      "# of image processed: 79000\n",
      "# of image processed: 80000\n",
      "# of image processed: 81000\n",
      "# of image processed: 82000\n",
      "# of image processed: 83000\n",
      "# of image processed: 84000\n",
      "# of image processed: 85000\n",
      "# of image processed: 86000\n",
      "# of image processed: 87000\n",
      "# of image processed: 88000\n",
      "# of image processed: 89000\n",
      "# of image processed: 90000\n",
      "# of image processed: 91000\n",
      "# of image processed: 92000\n",
      "# of image processed: 93000\n",
      "# of image processed: 94000\n",
      "# of image processed: 95000\n",
      "# of image processed: 96000\n",
      "# of image processed: 97000\n",
      "# of image processed: 98000\n",
      "# of image processed: 99000\n",
      "# of image processed: 100000\n",
      "# of image processed: 101000\n",
      "# of image processed: 102000\n",
      "# of image processed: 103000\n",
      "# of image processed: 104000\n",
      "# of image processed: 105000\n",
      "# of image processed: 106000\n",
      "# of image processed: 107000\n",
      "# of image processed: 108000\n",
      "# of image processed: 109000\n",
      "# of image processed: 110000\n",
      "# of image processed: 111000\n",
      "# of image processed: 112000\n",
      "# of image processed: 113000\n",
      "# of image processed: 114000\n",
      "# of image processed: 115000\n",
      "# of image processed: 116000\n",
      "# of image processed: 117000\n",
      "# of image processed: 118000\n",
      "Switching to another folder\n",
      "# of image processed: 119000\n",
      "# of image processed: 120000\n",
      "# of image processed: 121000\n",
      "# of image processed: 122000\n",
      "# of image processed: 123000\n",
      "# of image processed: 124000\n",
      "# of image processed: 125000\n",
      "# of image processed: 126000\n",
      "# of image processed: 127000\n",
      "# of image processed: 128000\n",
      "# of image processed: 129000\n",
      "# of image processed: 130000\n",
      "# of image processed: 131000\n",
      "# of image processed: 132000\n",
      "# of image processed: 133000\n",
      "# of image processed: 134000\n",
      "# of image processed: 135000\n",
      "# of image processed: 136000\n",
      "# of image processed: 137000\n",
      "# of image processed: 138000\n",
      "# of image processed: 139000\n",
      "# of image processed: 140000\n",
      "# of image processed: 141000\n",
      "# of image processed: 142000\n",
      "# of image processed: 143000\n",
      "# of image processed: 144000\n",
      "# of image processed: 145000\n",
      "# of image processed: 146000\n",
      "# of image processed: 147000\n",
      "# of image processed: 148000\n",
      "# of image processed: 149000\n",
      "# of image processed: 150000\n",
      "# of image processed: 151000\n",
      "# of image processed: 152000\n",
      "# of image processed: 153000\n",
      "# of image processed: 154000\n",
      "# of image processed: 155000\n",
      "# of image processed: 156000\n",
      "# of image processed: 157000\n",
      "# of image processed: 158000\n",
      "158957 images successfully processed\n"
     ]
    }
   ],
   "source": [
    "for modelPath in [\"./saved_models/model_0_inferenceModel.json\", \"./saved_models/model_1_inferenceModel.json\", \"./saved_models/model_5_inferenceModel.json\"]:\n",
    "    print(\"Using inference model:\", modelPath)\n",
    "    inferenceProcessor(\n",
    "        jsonModel=modelPath,\n",
    "        fileOutput=\"./\"+modelPath[len(\"./saved_models/\"):len(\"./saved_models/model_0\")]+\"_inferResult_bound.txt\",\n",
    "        imgFolderSource=[\"./train2017/\", \"./test2017/\"],\n",
    "        boundBox=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
