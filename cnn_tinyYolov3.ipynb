{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tensorflow version 2.1.0\n",
      "Keras is running on tensorflow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "from PIL import Image\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow.python.keras as keras\n",
    "from keras.layers import Input, Layer, LeakyReLU, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Add, Lambda\n",
    "from keras.models import Model, model_from_json, load_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import TerminateOnNaN, ModelCheckpoint, Callback\n",
    "import keras.backend as K\n",
    "import os\n",
    "K.clear_session()\n",
    "print(\"Running Tensorflow version\", tf.__version__)\n",
    "print(\"Keras is running on\", K.backend(), \"backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom layer classes successfully defined\n"
     ]
    }
   ],
   "source": [
    "# problem with model with rounding \n",
    "'''\n",
    "def roundingAlgo(x): \n",
    "    # first one that works with model_1 & model_2 \n",
    "    # problem - this rounding function is slow: model_2 = 3 hours / epoch\n",
    "    # comparison, model_0 = 20 mins / epoch\n",
    "    # in addition, off by half with integer inputs (lower than actual value, e.g. floor(2) ≈ 1.5, floor(2.01) ≈ 2)\n",
    "    # source: https://en.wikipedia.org/wiki/Floor_and_ceiling_functions#Continuity_and_series_expansions\n",
    "    if True:\n",
    "        result = x - 0.5\n",
    "        for p in range(1, 7):\n",
    "            result = result + K.sin(x * p * 2 * math.pi) / (p * math.pi)\n",
    "    return result\n",
    "# '''\n",
    "'''     \n",
    "def roundingAlgo(x):\n",
    "    # second one that works with model_2 \n",
    "    # problem - this rounding function is slower than first working algo: model_2 = 4,2 hours / epoch\n",
    "    # comparison, model_0 = 20 mins / epoch\n",
    "    # source: self\n",
    "    return x - x % 1\n",
    "# '''\n",
    "# '''\n",
    "def roundingAlgo(x): \n",
    "    # simplification of the first algo loop by simplifying the expression for range(1,7)\n",
    "    # problem - rounding function is still slow = 2,5 hours / epoch\n",
    "    # all non-speed problem of first algo still applies\n",
    "    result = x - 0.5\n",
    "    resultCos = K.cos(2 * math.pi * x)\n",
    "    return result + K.sin(2 * math.pi * x) * (1 + resultCos) * (13 + 2 * resultCos - 18 * K.pow(resultCos, 2) - 32 * K.pow(resultCos, 3) + 80 * K.pow(resultCos, 4)) / 15\n",
    "# '''\n",
    "'''\n",
    "def roundingAlgo(x): \n",
    "    # made to fool the engine to have a gradient\n",
    "    return 0 * x + K.round(x)\n",
    "# '''\n",
    "\n",
    "\n",
    "# check https://github.com/keras-team/keras/issues/2218\n",
    "# check https://github.com/keras-team/keras/issues/2221\n",
    "# https://www.tensorflow.org/api_docs/python/tf/custom_gradient\n",
    "class RoundClampQ7_12(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundClampQ7_12, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundClampQ7_12, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return K.clip(roundingAlgo(X * 4096), -524288, 524287) / 4096.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundClampQ7_12, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ7_12(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ7_12, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ7_12, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 4096) + 524288) % 1048576) - 524288) / 4096.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ7_12, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundClampQ3_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundClampQ3_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundClampQ3_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return K.clip(roundingAlgo(X * 16), -128, 127) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundClampQ3_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class RoundOverflowQ3_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoundOverflowQ3_4, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def build(self, input_shape):\n",
    "        super(RoundOverflowQ3_4, self).build(input_shape)\n",
    "    def call(self, X):\n",
    "        return (((roundingAlgo(X * 16) + 128) % 256) - 128) / 16.0\n",
    "    def get_config(self):\n",
    "        base_config = super(RoundOverflowQ3_4, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class Identity(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Identity, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def call(self, X):\n",
    "        return X\n",
    "    def get_config(self):\n",
    "        base_config = super(Identity, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "class IdentityFinalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(IdentityFinalLayer, self).__init__(**kwargs)\n",
    "        self.trainable = False\n",
    "    def call(self, X):\n",
    "        return X\n",
    "    def get_config(self):\n",
    "        base_config = super(IdentityFinalLayer, self).get_config()\n",
    "        return dict(list(base_config.items()))\n",
    "    \n",
    "def DBL(previousLayer, layerFilter, kernelSize=(3, 3), roundingFunction=Identity, name=None):\n",
    "    placeholder = \"\"\n",
    "    if name is not None:\n",
    "        placeholder = str(name)\n",
    "    else:\n",
    "        placeholder = str(time.time_ns())\n",
    "    return roundingFunction(name=\"ThirdRound_\"+placeholder)(\n",
    "        LeakyReLU(alpha=0.1)(\n",
    "            roundingFunction(name=\"SecondRound_\"+placeholder)(\n",
    "                BatchNormalization(name=\"BatchNorm_\"+placeholder)(\n",
    "                    roundingFunction(name=\"FirstRound_\"+placeholder)(\n",
    "                        Conv2D(filters=layerFilter, kernel_size=kernelSize, padding='same', use_bias=False, kernel_regularizer=l2(5e-4), name=\"Conv2D_\"+placeholder)(\n",
    "                            previousLayer\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "print(\"Custom layer classes successfully defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class classification is 80\n"
     ]
    }
   ],
   "source": [
    "classificationClass = 80\n",
    "\n",
    "print(\"Number of class classification is\", classificationClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target data generator successfully defined\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand()*(b-a) + a\n",
    "\n",
    "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n",
    "    '''\n",
    "    random preprocessing for real-time data augmentation \n",
    "    \n",
    "    random=True induces image processing (better data accuracy with cost of cycles)\n",
    "    '''\n",
    "    line = annotation_line.split()\n",
    "    image = Image.open(line[0])\n",
    "    iw, ih = image.size\n",
    "    h, w = input_shape\n",
    "    box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
    "\n",
    "    if not random:\n",
    "        # resize image\n",
    "        scale = min(w/iw, h/ih)\n",
    "        nw = int(iw*scale)\n",
    "        nh = int(ih*scale)\n",
    "        dx = (w-nw)//2\n",
    "        dy = (h-nh)//2\n",
    "        image_data=0\n",
    "        if proc_img:\n",
    "            image = image.resize((nw,nh), Image.BICUBIC)\n",
    "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "            new_image.paste(image, (dx, dy))\n",
    "            image_data = np.array(new_image)/255.\n",
    "\n",
    "        # correct boxes\n",
    "        box_data = np.zeros((max_boxes,5))\n",
    "        if len(box)>0:\n",
    "            np.random.shuffle(box)\n",
    "            if len(box)>max_boxes: box = box[:max_boxes]\n",
    "            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n",
    "            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n",
    "            box_data[:len(box)] = box\n",
    "\n",
    "        return image_data, box_data\n",
    "\n",
    "    # resize image\n",
    "    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n",
    "    scale = rand(.25, 2)\n",
    "    if new_ar < 1:\n",
    "        nh = int(scale*h)\n",
    "        nw = int(nh*new_ar)\n",
    "    else:\n",
    "        nw = int(scale*w)\n",
    "        nh = int(nw/new_ar)\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "\n",
    "    # place image\n",
    "    dx = int(rand(0, w-nw))\n",
    "    dy = int(rand(0, h-nh))\n",
    "    new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "    new_image.paste(image, (dx, dy))\n",
    "    image = new_image\n",
    "\n",
    "    # flip image or not\n",
    "    flip = rand()<.5\n",
    "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # distort image\n",
    "    hue = rand(-hue, hue)\n",
    "    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
    "    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
    "    x = rgb_to_hsv(np.array(image)/255.)\n",
    "    x[..., 0] += hue\n",
    "    x[..., 0][x[..., 0]>1] -= 1\n",
    "    x[..., 0][x[..., 0]<0] += 1\n",
    "    x[..., 1] *= sat\n",
    "    x[..., 2] *= val\n",
    "    x[x>1] = 1\n",
    "    x[x<0] = 0\n",
    "    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n",
    "\n",
    "    # correct boxes\n",
    "    box_data = np.zeros((max_boxes,5))\n",
    "    if len(box)>0:\n",
    "        np.random.shuffle(box)\n",
    "        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
    "        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
    "        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
    "        box[:, 0:2][box[:, 0:2]<0] = 0\n",
    "        box[:, 2][box[:, 2]>w] = w\n",
    "        box[:, 3][box[:, 3]>h] = h\n",
    "        box_w = box[:, 2] - box[:, 0]\n",
    "        box_h = box[:, 3] - box[:, 1]\n",
    "        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
    "        if len(box)>max_boxes: box = box[:max_boxes]\n",
    "        box_data[:len(box)] = box\n",
    "\n",
    "    return image_data, box_data\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n",
    "    '''Preprocess true boxes to training input format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: array, shape=(m, T, 5)\n",
    "        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n",
    "    input_shape: array-like, hw, multiples of 32\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n",
    "\n",
    "    '''\n",
    "    assert (true_boxes[..., 4]<num_classes).all(), 'class id must be less than num_classes'\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "\n",
    "    true_boxes = np.array(true_boxes, dtype='float32')\n",
    "    input_shape = np.array(input_shape, dtype='int32')\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n",
    "    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\n",
    "    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\n",
    "\n",
    "    m = true_boxes.shape[0]\n",
    "    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n",
    "    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n",
    "        dtype='float32') for l in range(num_layers)]\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "    anchor_maxes = anchors / 2.\n",
    "    anchor_mins = -anchor_maxes\n",
    "    valid_mask = boxes_wh[..., 0]>0\n",
    "\n",
    "    for b in range(m):\n",
    "        # Discard zero rows.\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh)==0: continue\n",
    "        # Expand dim to apply broadcasting.\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        box_maxes = wh / 2.\n",
    "        box_mins = -box_maxes\n",
    "\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        box_area = wh[..., 0] * wh[..., 1]\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "\n",
    "        # Find best anchor for each true box\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "\n",
    "        for t, n in enumerate(best_anchor):\n",
    "            for l in range(num_layers):\n",
    "                if n in anchor_mask[l]:\n",
    "                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype('int32')\n",
    "                    k = anchor_mask[l].index(n)\n",
    "                    c = true_boxes[b,t, 4].astype('int32')\n",
    "                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n",
    "                    y_true[l][b, j, i, k, 4] = 1\n",
    "                    y_true[l][b, j, i, k, 5+c] = 1\n",
    "\n",
    "    return y_true\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i + 1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "print(\"Target data generator successfully defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diambil dari https://github.com/qqwweee/keras-yolo3\n",
    "'''\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 qqwweee\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "'''\n",
    "\n",
    "image_size = (416,416)\n",
    "image_height, image_width = image_size\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "tinyYolo_anchors = get_anchors(\"../CNN-VLSI/tiny_yolo_anchors.txt\")\n",
    "\n",
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "        [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "        [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    box_xy = (K.hard_sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n",
    "    box_confidence = K.hard_sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.hard_sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "def box_iou(b1, b2):\n",
    "    '''Return iou tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh/2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxes = b1_xy + b1_wh_half\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh/2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "\n",
    "    intersect_mins = K.maximum(b1_mins, b2_mins)\n",
    "    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n",
    "    '''Return yolo_loss tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "    y_true: list of array, the output of preprocess_true_boxes\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor, shape=(1,)\n",
    "\n",
    "    '''\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    yolo_outputs = args[:num_layers]\n",
    "    y_true = args[num_layers:]\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n",
    "    loss = 0\n",
    "    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\n",
    "    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "\n",
    "    for l in range(num_layers):\n",
    "        object_mask = y_true[l][..., 4:5]\n",
    "        true_class_probs = y_true[l][..., 5:]\n",
    "\n",
    "        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n",
    "        pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "\n",
    "        # Darknet raw box to calculate loss.\n",
    "        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\n",
    "        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n",
    "        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n",
    "        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\n",
    "\n",
    "        # Find ignore mask, iterate over each of batch.\n",
    "        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "        object_mask_bool = K.cast(object_mask, 'bool')\n",
    "        def loop_body(b, ignore_mask):\n",
    "            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n",
    "            iou = box_iou(pred_box[b], true_box)\n",
    "            best_iou = K.max(iou, axis=-1)\n",
    "            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n",
    "            return b+1, ignore_mask\n",
    "        _, ignore_mask = tf.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n",
    "        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n",
    "        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n",
    "            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n",
    "        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n",
    "\n",
    "        xy_loss = K.sum(xy_loss) / mf\n",
    "        wh_loss = K.sum(wh_loss) / mf\n",
    "        confidence_loss = K.sum(confidence_loss) / mf\n",
    "        class_loss = K.sum(class_loss) / mf\n",
    "        loss += xy_loss + wh_loss + confidence_loss + class_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data is 117266\n",
      "# of validation data is 4952\n"
     ]
    }
   ],
   "source": [
    "with open(\"../CNN-VLSI/train.txt\") as trainText:\n",
    "    train_annotation_lines = trainText.readlines()\n",
    "with open(\"../CNN-VLSI/val.txt\") as valText:\n",
    "    val_annotation_lines = valText.readlines()\n",
    "lenTrain = len(train_annotation_lines)\n",
    "print(\"# of training data is\", lenTrain)\n",
    "lenVal = len(val_annotation_lines)\n",
    "print(\"# of validation data is\", lenVal)\n",
    "np.random.shuffle(train_annotation_lines)\n",
    "np.random.shuffle(val_annotation_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_0 does no rounding (float32 operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_0 = None\n",
    "'''\n",
    "model_0_input = Input(shape=(None, None, 3), name=\"model_0_inputLayer\")\n",
    "# model_0_pointer = model_0_input\n",
    "print(\"Input shape:\", model_0_input.shape) # 448 x 448 x 3\n",
    "model_0_startBranch = DBL(roundingFunction=Identity, previousLayer=model_0_input, layerFilter=16, name=\"model_0_layer0_branch\") \n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 448 x 448 x 16\n",
    "model_0_startBranch = MaxPooling2D(pool_size=(2, 2))(model_0_startBranch)\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 224 x 224 x 16\n",
    "model_0_startBranch = DBL(roundingFunction=Identity, previousLayer=model_0_startBranch, layerFilter=32, name=\"model_0_layer1_branch\")\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 224 x 224 x 32\n",
    "model_0_startBranch = MaxPooling2D(pool_size=(2, 2))(model_0_startBranch)\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 112 x 112 x 32\n",
    "model_0_startBranch = DBL(roundingFunction=Identity, previousLayer=model_0_startBranch, layerFilter=64, name=\"model_0_layer2_branch\")\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 112 x 112 x 64\n",
    "model_0_startBranch = MaxPooling2D(pool_size=(2, 2))(model_0_startBranch)\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 56 x 56 x 64\n",
    "model_0_startBranch = DBL(roundingFunction=Identity, previousLayer=model_0_startBranch, layerFilter=128, name=\"model_0_layer3_branch\")\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 56 x 56 x 128\n",
    "model_0_startBranch = MaxPooling2D(pool_size=(2, 2))(model_0_startBranch)\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 28 x 28 x 128\n",
    "model_0_startBranch = DBL(roundingFunction=Identity, previousLayer=model_0_startBranch, layerFilter=256, name=\"model_0_layer4_branch\")\n",
    "# model_0_pointer = model_0_startBranch\n",
    "# print(model_0_pointer.shape) # 28 x 28 x 256\n",
    "print(\"Branch split from main branch - following branch 0\") # 2 branch split from startBranch (28 x 28 x 256), following model_0_branch0\n",
    "model_0_branch0 = MaxPooling2D(pool_size=(2, 2))(model_0_startBranch)\n",
    "# model_0_pointer = model_0_branch0\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 256\n",
    "model_0_branch0 = DBL(roundingFunction=Identity, previousLayer=model_0_branch0, layerFilter=512, name=\"model_0_layer5_branch0\")\n",
    "# model_0_pointer = model_0_branch0\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 512\n",
    "model_0_branch0 = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(model_0_branch0)\n",
    "# model_0_pointer = model_0_branch0\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 512\n",
    "model_0_branch0 = DBL(roundingFunction=Identity, previousLayer=model_0_branch0, layerFilter=1024, name=\"model_0_layer6_branch0\")\n",
    "# model_0_pointer = model_0_branch0\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 1024\n",
    "model_0_branch0 = DBL(roundingFunction=Identity, previousLayer=model_0_branch0, layerFilter=256, kernelSize=(1, 1), name=\"model_0_layer7_branch0\")\n",
    "# model_0_pointer = model_0_branch0\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 256\n",
    "print(\"Branch split from branch 0 - following branch 0,0\") # 2 branch split from model_0_branch0 (14 x 14 x 256), following model_0_branch00\n",
    "model_0_branch00 = DBL(roundingFunction=Identity, previousLayer=model_0_branch0, layerFilter=128, kernelSize=(1, 1), name=\"model_0_layer8_branch00\")\n",
    "# model_0_pointer = model_0_branch00\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 128\n",
    "model_0_branch00 = UpSampling2D()(model_0_branch00)\n",
    "# model_0_pointer = model_0_branch00\n",
    "# print(model_0_pointer.shape) # 28 x 28 x 128\n",
    "print(\"Branch merge from branch 1 and branch 0,0\") # 2 branch merge from model_0_branch1 (unchanged from model_0_startBranch) and model_0_branch00\n",
    "model_0_mergedBranch = Concatenate()([model_0_startBranch, model_0_branch00])\n",
    "# model_0_pointer = model_0_mergedBranch\n",
    "# print(model_0_pointer.shape) # 28 x 28 x 384\n",
    "model_0_mergedBranch = DBL(roundingFunction=Identity, previousLayer=model_0_mergedBranch, layerFilter=256, name=\"model_0_layer9_branch1\")\n",
    "# model_0_pointer = model_0_mergedBranch\n",
    "# print(model_0_pointer.shape) # 28 x 28 x 256\n",
    "model_0_mergedBranch = DBL(roundingFunction=Identity, previousLayer=model_0_mergedBranch, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_0_layerA_branch1\")\n",
    "# model_0_pointer = model_0_mergedBranch\n",
    "print(\"Model output 1 shape:\", model_0_mergedBranch.shape) # 28 x 28 x (3 * (5 + classificationClass))\n",
    "model_0_mergedBranch = IdentityFinalLayer(name=\"model_0_outputLayer_1\")(model_0_mergedBranch)\n",
    "print() # OUTPUT = model_0_mergedBranch (note: 26 x 26 grid untuk deteksi objek kecil)\n",
    "\n",
    "print(\"Branch split from branch 0 - following branch 0,1\")# following model_0_branch01\n",
    "model_0_branch01 = DBL(roundingFunction=Identity, previousLayer=model_0_branch0, layerFilter=512, name=\"model_0_layer8_branch01\")\n",
    "# model_0_pointer = model_0_branch01\n",
    "# print(model_0_pointer.shape) # 14 x 14 x 512\n",
    "model_0_branch01 = DBL(roundingFunction=Identity, previousLayer=model_0_branch01, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_0_layer9_branch01\")\n",
    "# model_0_pointer = model_0_branch01\n",
    "print(\"Model output 0 shape:\", model_0_branch01.shape) # 14 x 14 x (3 * (5 + classificationClass))\n",
    "model_0_branch01 = IdentityFinalLayer(name=\"model_0_outputLayer_0\")(model_0_branch01)\n",
    "print() # OUTPUT = model_0_branch01 (note: 13 x 13 grid untuk deteksi objek besar)\n",
    "\n",
    "model_0_actual = Model(inputs=model_0_input, outputs=[model_0_branch01, model_0_mergedBranch]) # mengikuti model dari https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "\n",
    "with open(\"./saved_models/model_0_inferenceModel.json\", \"wt\") as jsonFile:\n",
    "    jsonFile.write(model_0_actual.to_json())\n",
    "\n",
    "try:\n",
    "#     model_0_actual.load_weights(\"../keras-yolo3/model_data/yolov3_tiny_roundOverflowQ3_4.h5\", by_name=True, skip_mismatch=True)\n",
    "    model_0_actual.load_weights(\"./saved_models/model_0_checkpoint.h5\", by_name=True, skip_mismatch=True)\n",
    "    print(\"Weight load attempt success for model_0\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load existing model for model_0:\", e)\n",
    "try:\n",
    "    model_0_actual.save_weights(\"./saved_models/model_0_trainModel.h5\")\n",
    "    print(\"Loaded model is successfully re-saved\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save loaded model:\", e)\n",
    "# added code from https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "setArgs_model_0 = {\n",
    "    'anchors': tinyYolo_anchors, \n",
    "    'num_classes': classificationClass, \n",
    "    'ignore_thresh': 0.2\n",
    "}\n",
    "\n",
    "y_true_model_0 = [Input(shape=(image_height//{0:32, 1:16}[l], image_width//{0:32, 1:16}[l], 3, classificationClass + 5)) for l in range(2)]\n",
    "\n",
    "model_0_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss_model_0', arguments=setArgs_model_0)([*model_0_actual.output, *y_true_model_0])\n",
    "\n",
    "model_0 = Model([model_0_actual.input, *y_true_model_0], model_0_loss)\n",
    "\n",
    "# model_0_actual adalah working model yang akan dipakai untuk inference\n",
    "# y_true adalah layer input yang akan diisikan nilai sebenarnya\n",
    "# cara training model_0 adalah pendekatan sehingga output model_0 sedekat mungkin dengan 0 (model_0 ≈ model_0_actual - y_true)\n",
    "\n",
    "model_0.compile(optimizer=Adam(lr=1e-2), loss={'yolo_loss_model_0': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# loss adalah seberapa jauh nilai perbedaan output dengan 0\n",
    "\n",
    "print(\"Model model_0 compilation complete\") # tinggal porting line 54 sampai 61 dari https://github.com/awe777/keras-yolo3/blob/master/train.py, dilakukan di cell training di bawah\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1 approximates Q7.12 signed fixed point operations with floating point rules (overflow = maximum/minimum value) \n",
    "# Done by rounding to the nearest 1/4096 and capping at [-128, 128) after batch normalization and activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = None\n",
    "'''\n",
    "model_1_input = Input(shape=(None, None, 3), name=\"model_1_inputLayer\")\n",
    "# model_1_pointer = model_1_input\n",
    "print(\"Input shape:\", model_1_input.shape) # 448 x 448 x 3\n",
    "model_1_startBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_input, layerFilter=16, name=\"model_1_layer0_branch\") \n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 448 x 448 x 16\n",
    "model_1_startBranch = MaxPooling2D(pool_size=(2, 2))(model_1_startBranch)\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 224 x 224 x 16\n",
    "model_1_startBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_startBranch, layerFilter=32, name=\"model_1_layer1_branch\")\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 224 x 224 x 32\n",
    "model_1_startBranch = MaxPooling2D(pool_size=(2, 2))(model_1_startBranch)\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 112 x 112 x 32\n",
    "model_1_startBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_startBranch, layerFilter=64, name=\"model_1_layer2_branch\")\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 112 x 112 x 64\n",
    "model_1_startBranch = MaxPooling2D(pool_size=(2, 2))(model_1_startBranch)\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 56 x 56 x 64\n",
    "model_1_startBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_startBranch, layerFilter=128, name=\"model_1_layer3_branch\")\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 56 x 56 x 128\n",
    "model_1_startBranch = MaxPooling2D(pool_size=(2, 2))(model_1_startBranch)\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 28 x 28 x 128\n",
    "model_1_startBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_startBranch, layerFilter=256, name=\"model_1_layer4_branch\")\n",
    "# model_1_pointer = model_1_startBranch\n",
    "# print(model_1_pointer.shape) # 28 x 28 x 256\n",
    "print(\"Branch split from main branch - following branch 0\") # 2 branch split from startBranch (28 x 28 x 256), following model_1_branch0\n",
    "model_1_branch0 = MaxPooling2D(pool_size=(2, 2))(model_1_startBranch)\n",
    "# model_1_pointer = model_1_branch0\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 256\n",
    "model_1_branch0 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch0, layerFilter=512, name=\"model_1_layer5_branch0\")\n",
    "# model_1_pointer = model_1_branch0\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 512\n",
    "model_1_branch0 = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(model_1_branch0)\n",
    "# model_1_pointer = model_1_branch0\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 512\n",
    "model_1_branch0 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch0, layerFilter=1024, name=\"model_1_layer6_branch0\")\n",
    "# model_1_pointer = model_1_branch0\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 1024\n",
    "model_1_branch0 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch0, layerFilter=256, kernelSize=(1, 1), name=\"model_1_layer7_branch0\")\n",
    "# model_1_pointer = model_1_branch0\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 256\n",
    "print(\"Branch split from branch 0 - following branch 0,0\") # 2 branch split from model_1_branch0 (14 x 14 x 256), following model_1_branch00\n",
    "model_1_branch00 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch0, layerFilter=128, kernelSize=(1, 1), name=\"model_1_layer8_branch00\")\n",
    "# model_1_pointer = model_1_branch00\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 128\n",
    "model_1_branch00 = UpSampling2D()(model_1_branch00)\n",
    "# model_1_pointer = model_1_branch00\n",
    "# print(model_1_pointer.shape) # 28 x 28 x 128\n",
    "print(\"Branch merge from branch 1 and branch 0,0\") # 2 branch merge from model_1_branch1 (unchanged from model_1_startBranch) and model_1_branch00\n",
    "model_1_mergedBranch = Concatenate()([model_1_startBranch, model_1_branch00])\n",
    "# model_1_pointer = model_1_mergedBranch\n",
    "# print(model_1_pointer.shape) # 28 x 28 x 384\n",
    "model_1_mergedBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_mergedBranch, layerFilter=256, name=\"model_1_layer9_branch1\")\n",
    "# model_1_pointer = model_1_mergedBranch\n",
    "# print(model_1_pointer.shape) # 28 x 28 x 256\n",
    "model_1_mergedBranch = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_mergedBranch, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_1_layerA_branch1\")\n",
    "# model_1_pointer = model_1_mergedBranch\n",
    "print(\"Model output 1 shape:\", model_1_mergedBranch.shape) # 28 x 28 x (3 * (5 + classificationClass))\n",
    "model_1_mergedBranch = IdentityFinalLayer(name=\"model_1_outputLayer_1\")(model_1_mergedBranch)\n",
    "print() # OUTPUT = model_1_mergedBranch (note: 26 x 26 grid untuk deteksi objek kecil)\n",
    "\n",
    "print(\"Branch split from branch 0 - following branch 0,1\")# following model_1_branch01\n",
    "model_1_branch01 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch0, layerFilter=512, name=\"model_1_layer8_branch01\")\n",
    "# model_1_pointer = model_1_branch01\n",
    "# print(model_1_pointer.shape) # 14 x 14 x 512\n",
    "model_1_branch01 = DBL(roundingFunction=RoundClampQ7_12, previousLayer=model_1_branch01, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_1_layer9_branch01\")\n",
    "# model_1_pointer = model_1_branch01\n",
    "print(\"Model output 0 shape:\", model_1_branch01.shape) # 14 x 14 x (3 * (5 + classificationClass))\n",
    "model_1_branch01 = IdentityFinalLayer(name=\"model_1_outputLayer_0\")(model_1_branch01)\n",
    "print() # OUTPUT = model_1_branch01 (note: 13 x 13 grid untuk deteksi objek besar)\n",
    "\n",
    "model_1_actual = Model(inputs=model_1_input, outputs=[model_1_branch01, model_1_mergedBranch]) # mengikuti model dari https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "\n",
    "with open(\"./saved_models/model_1_inferenceModel.json\", \"wt\") as jsonFile:\n",
    "    jsonFile.write(model_1_actual.to_json())\n",
    "\n",
    "try:\n",
    "#     model_1_actual.load_weights(\"../keras-yolo3/model_data/yolov3_tiny_roundOverflowQ3_4.h5\", by_name=True, skip_mismatch=True)\n",
    "    model_1_actual.load_weights(\"./saved_models/model_1_checkpoint.h5\", by_name=True, skip_mismatch=True)\n",
    "    print(\"Weight load attempt success for model_1\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load existing model for model_1:\", e)\n",
    "try:\n",
    "    model_1_actual.save_weights(\"./saved_models/model_1_trainModel.h5\")\n",
    "    print(\"Loaded model is successfully re-saved\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save loaded model:\", e)\n",
    "# added code from https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "setArgs_model_1 = {\n",
    "    'anchors': tinyYolo_anchors, \n",
    "    'num_classes': classificationClass, \n",
    "    'ignore_thresh': 0.2\n",
    "}\n",
    "\n",
    "y_true_model_1 = [Input(shape=(image_height//{0:32, 1:16}[l], image_width//{0:32, 1:16}[l], 3, classificationClass + 5)) for l in range(2)]\n",
    "\n",
    "model_1_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss_model_1', arguments=setArgs_model_1)([*model_1_actual.output, *y_true_model_1])\n",
    "\n",
    "model_1 = Model([model_1_actual.input, *y_true_model_1], model_1_loss)\n",
    "\n",
    "# model_1_actual adalah working model yang akan dipakai untuk inference\n",
    "# y_true adalah layer input yang akan diisikan nilai sebenarnya\n",
    "# cara training model_1 adalah pendekatan sehingga output model_1 sedekat mungkin dengan 0 (model_1 ≈ model_1_actual - y_true)\n",
    "\n",
    "model_1.compile(optimizer=Adam(lr=1e-2), loss={'yolo_loss_model_1': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# loss adalah seberapa jauh nilai perbedaan output dengan 0\n",
    "\n",
    "print(\"Model model_1 compilation complete\") # tinggal porting line 54 sampai 61 dari https://github.com/awe777/keras-yolo3/blob/master/train.py, dilakukan di cell training di bawah\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 approximates Q7.12 signed fixed point operations with integer rules (overflow = positive -> negative & vice versa) \n",
    "# Done by rounding to the nearest 1/4096 and capping at [-128, 128) after batch normalization and activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_2 = None\n",
    "'''\n",
    "model_2_input = Input(shape=(None, None, 3), name=\"model_2_inputLayer\")\n",
    "# model_2_pointer = model_2_input\n",
    "print(\"Input shape:\", model_2_input.shape) # 448 x 448 x 3\n",
    "model_2_startBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_input, layerFilter=16, name=\"model_2_layer0_branch\") \n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 448 x 448 x 16\n",
    "model_2_startBranch = MaxPooling2D(pool_size=(2, 2))(model_2_startBranch)\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 224 x 224 x 16\n",
    "model_2_startBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_startBranch, layerFilter=32, name=\"model_2_layer1_branch\")\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 224 x 224 x 32\n",
    "model_2_startBranch = MaxPooling2D(pool_size=(2, 2))(model_2_startBranch)\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 112 x 112 x 32\n",
    "model_2_startBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_startBranch, layerFilter=64, name=\"model_2_layer2_branch\")\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 112 x 112 x 64\n",
    "model_2_startBranch = MaxPooling2D(pool_size=(2, 2))(model_2_startBranch)\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 56 x 56 x 64\n",
    "model_2_startBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_startBranch, layerFilter=128, name=\"model_2_layer3_branch\")\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 56 x 56 x 128\n",
    "model_2_startBranch = MaxPooling2D(pool_size=(2, 2))(model_2_startBranch)\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 28 x 28 x 128\n",
    "model_2_startBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_startBranch, layerFilter=256, name=\"model_2_layer4_branch\")\n",
    "# model_2_pointer = model_2_startBranch\n",
    "# print(model_2_pointer.shape) # 28 x 28 x 256\n",
    "print(\"Branch split from main branch - following branch 0\") # 2 branch split from startBranch (28 x 28 x 256), following model_2_branch0\n",
    "model_2_branch0 = MaxPooling2D(pool_size=(2, 2))(model_2_startBranch)\n",
    "# model_2_pointer = model_2_branch0\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 256\n",
    "model_2_branch0 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch0, layerFilter=512, name=\"model_2_layer5_branch0\")\n",
    "# model_2_pointer = model_2_branch0\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 512\n",
    "model_2_branch0 = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(model_2_branch0)\n",
    "# model_2_pointer = model_2_branch0\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 512\n",
    "model_2_branch0 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch0, layerFilter=1024, name=\"model_2_layer6_branch0\")\n",
    "# model_2_pointer = model_2_branch0\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 1024\n",
    "model_2_branch0 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch0, layerFilter=256, kernelSize=(1, 1), name=\"model_2_layer7_branch0\")\n",
    "# model_2_pointer = model_2_branch0\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 256\n",
    "print(\"Branch split from branch 0 - following branch 0,0\") # 2 branch split from model_2_branch0 (14 x 14 x 256), following model_2_branch00\n",
    "model_2_branch00 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch0, layerFilter=128, kernelSize=(1, 1), name=\"model_2_layer8_branch00\")\n",
    "# model_2_pointer = model_2_branch00\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 128\n",
    "model_2_branch00 = UpSampling2D()(model_2_branch00)\n",
    "# model_2_pointer = model_2_branch00\n",
    "# print(model_2_pointer.shape) # 28 x 28 x 128\n",
    "print(\"Branch merge from branch 1 and branch 0,0\") # 2 branch merge from model_2_branch1 (unchanged from model_2_startBranch) and model_2_branch00\n",
    "model_2_mergedBranch = Concatenate()([model_2_startBranch, model_2_branch00])\n",
    "# model_2_pointer = model_2_mergedBranch\n",
    "# print(model_2_pointer.shape) # 28 x 28 x 384\n",
    "model_2_mergedBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_mergedBranch, layerFilter=256, name=\"model_2_layer9_branch1\")\n",
    "# model_2_pointer = model_2_mergedBranch\n",
    "# print(model_2_pointer.shape) # 28 x 28 x 256\n",
    "model_2_mergedBranch = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_mergedBranch, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_2_layerA_branch1\")\n",
    "# model_2_pointer = model_2_mergedBranch\n",
    "print(\"Model output 1 shape:\", model_2_mergedBranch.shape) # 28 x 28 x (3 * (5 + classificationClass))\n",
    "model_2_mergedBranch = IdentityFinalLayer(name=\"model_2_outputLayer_1\")(model_2_mergedBranch)\n",
    "print() # OUTPUT = model_2_mergedBranch (note: 26 x 26 grid untuk deteksi objek kecil)\n",
    "\n",
    "print(\"Branch split from branch 0 - following branch 0,1\")# following model_2_branch01\n",
    "model_2_branch01 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch0, layerFilter=512, name=\"model_2_layer8_branch01\")\n",
    "# model_2_pointer = model_2_branch01\n",
    "# print(model_2_pointer.shape) # 14 x 14 x 512\n",
    "model_2_branch01 = DBL(roundingFunction=RoundOverflowQ7_12, previousLayer=model_2_branch01, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_2_layer9_branch01\")\n",
    "# model_2_pointer = model_2_branch01\n",
    "print(\"Model output 0 shape:\", model_2_branch01.shape) # 14 x 14 x (3 * (5 + classificationClass))\n",
    "model_2_branch01 = IdentityFinalLayer(name=\"model_2_outputLayer_0\")(model_2_branch01)\n",
    "print() # OUTPUT = model_2_branch01 (note: 13 x 13 grid untuk deteksi objek besar)\n",
    "\n",
    "model_2_actual = Model(inputs=model_2_input, outputs=[model_2_branch01, model_2_mergedBranch]) # mengikuti model dari https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "\n",
    "with open(\"./saved_models/model_2_inferenceModel.json\", \"wt\") as jsonFile:\n",
    "    jsonFile.write(model_2_actual.to_json())\n",
    "\n",
    "try:\n",
    "#     model_2_actual.load_weights(\"../keras-yolo3/model_data/yolov3_tiny_roundOverflowQ3_4.h5\", by_name=True, skip_mismatch=True)\n",
    "    model_2_actual.load_weights(\"./saved_models/model_2_checkpoint.h5\", by_name=True, skip_mismatch=True)\n",
    "    print(\"Weight load attempt success for model_2\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load existing model for model_2:\", e)\n",
    "try:\n",
    "    model_2_actual.save_weights(\"./saved_models/model_2_trainModel.h5\")\n",
    "    print(\"Loaded model is successfully re-saved\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save loaded model:\", e)\n",
    "# added code from https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "setArgs_model_2 = {\n",
    "    'anchors': tinyYolo_anchors, \n",
    "    'num_classes': classificationClass, \n",
    "    'ignore_thresh': 0.2\n",
    "}\n",
    "\n",
    "y_true_model_2 = [Input(shape=(image_height//{0:32, 1:16}[l], image_width//{0:32, 1:16}[l], 3, classificationClass + 5)) for l in range(2)]\n",
    "\n",
    "model_2_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss_model_2', arguments=setArgs_model_2)([*model_2_actual.output, *y_true_model_2])\n",
    "\n",
    "model_2 = Model([model_2_actual.input, *y_true_model_2], model_2_loss)\n",
    "\n",
    "# model_2_actual adalah working model yang akan dipakai untuk inference\n",
    "# y_true adalah layer input yang akan diisikan nilai sebenarnya\n",
    "# cara training model_2 adalah pendekatan sehingga output model_2 sedekat mungkin dengan 0 (model_2 ≈ model_2_actual - y_true)\n",
    "\n",
    "model_2.compile(optimizer=Adam(lr=1e-2), loss={'yolo_loss_model_2': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# loss adalah seberapa jauh nilai perbedaan output dengan 0\n",
    "\n",
    "print(\"Model model_2 compilation complete\") # tinggal porting line 54 sampai 61 dari https://github.com/awe777/keras-yolo3/blob/master/train.py, dilakukan di cell training di bawah\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3 approximates Q3.4 signed fixed point operations with floating point rules (overflow = maximum/minimum value) \n",
    "# Done by rounding to the nearest 1/16 and capping at [-8, 8) after batch normalization and activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_3 = None\n",
    "'''\n",
    "model_3_input = Input(shape=(None, None, 3), name=\"model_3_inputLayer\")\n",
    "# model_3_pointer = model_3_input\n",
    "print(\"Input shape:\", model_3_input.shape) # 448 x 448 x 3\n",
    "model_3_startBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_input, layerFilter=16, name=\"model_3_layer0_branch\") \n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 448 x 448 x 16\n",
    "model_3_startBranch = MaxPooling2D(pool_size=(2, 2))(model_3_startBranch)\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 224 x 224 x 16\n",
    "model_3_startBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_startBranch, layerFilter=32, name=\"model_3_layer1_branch\")\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 224 x 224 x 32\n",
    "model_3_startBranch = MaxPooling2D(pool_size=(2, 2))(model_3_startBranch)\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 112 x 112 x 32\n",
    "model_3_startBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_startBranch, layerFilter=64, name=\"model_3_layer2_branch\")\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 112 x 112 x 64\n",
    "model_3_startBranch = MaxPooling2D(pool_size=(2, 2))(model_3_startBranch)\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 56 x 56 x 64\n",
    "model_3_startBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_startBranch, layerFilter=128, name=\"model_3_layer3_branch\")\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 56 x 56 x 128\n",
    "model_3_startBranch = MaxPooling2D(pool_size=(2, 2))(model_3_startBranch)\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 28 x 28 x 128\n",
    "model_3_startBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_startBranch, layerFilter=256, name=\"model_3_layer4_branch\")\n",
    "# model_3_pointer = model_3_startBranch\n",
    "# print(model_3_pointer.shape) # 28 x 28 x 256\n",
    "print(\"Branch split from main branch - following branch 0\") # 2 branch split from startBranch (28 x 28 x 256), following model_3_branch0\n",
    "model_3_branch0 = MaxPooling2D(pool_size=(2, 2))(model_3_startBranch)\n",
    "# model_3_pointer = model_3_branch0\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 256\n",
    "model_3_branch0 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch0, layerFilter=512, name=\"model_3_layer5_branch0\")\n",
    "# model_3_pointer = model_3_branch0\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 512\n",
    "model_3_branch0 = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(model_3_branch0)\n",
    "# model_3_pointer = model_3_branch0\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 512\n",
    "model_3_branch0 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch0, layerFilter=1024, name=\"model_3_layer6_branch0\")\n",
    "# model_3_pointer = model_3_branch0\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 1024\n",
    "model_3_branch0 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch0, layerFilter=256, kernelSize=(1, 1), name=\"model_3_layer7_branch0\")\n",
    "# model_3_pointer = model_3_branch0\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 256\n",
    "print(\"Branch split from branch 0 - following branch 0,0\") # 2 branch split from model_3_branch0 (14 x 14 x 256), following model_3_branch00\n",
    "model_3_branch00 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch0, layerFilter=128, kernelSize=(1, 1), name=\"model_3_layer8_branch00\")\n",
    "# model_3_pointer = model_3_branch00\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 128\n",
    "model_3_branch00 = UpSampling2D()(model_3_branch00)\n",
    "# model_3_pointer = model_3_branch00\n",
    "# print(model_3_pointer.shape) # 28 x 28 x 128\n",
    "print(\"Branch merge from branch 1 and branch 0,0\") # 2 branch merge from model_3_branch1 (unchanged from model_3_startBranch) and model_3_branch00\n",
    "model_3_mergedBranch = Concatenate()([model_3_startBranch, model_3_branch00])\n",
    "# model_3_pointer = model_3_mergedBranch\n",
    "# print(model_3_pointer.shape) # 28 x 28 x 384\n",
    "model_3_mergedBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_mergedBranch, layerFilter=256, name=\"model_3_layer9_branch1\")\n",
    "# model_3_pointer = model_3_mergedBranch\n",
    "# print(model_3_pointer.shape) # 28 x 28 x 256\n",
    "model_3_mergedBranch = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_mergedBranch, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_3_layerA_branch1\")\n",
    "# model_3_pointer = model_3_mergedBranch\n",
    "print(\"Model output 1 shape:\", model_3_mergedBranch.shape) # 28 x 28 x (3 * (5 + classificationClass))\n",
    "model_3_mergedBranch = IdentityFinalLayer(name=\"model_3_outputLayer_1\")(model_3_mergedBranch)\n",
    "print() # OUTPUT = model_3_mergedBranch (note: 26 x 26 grid untuk deteksi objek kecil)\n",
    "\n",
    "print(\"Branch split from branch 0 - following branch 0,1\")# following model_3_branch01\n",
    "model_3_branch01 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch0, layerFilter=512, name=\"model_3_layer8_branch01\")\n",
    "# model_3_pointer = model_3_branch01\n",
    "# print(model_3_pointer.shape) # 14 x 14 x 512\n",
    "model_3_branch01 = DBL(roundingFunction=RoundClampQ3_4, previousLayer=model_3_branch01, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_3_layer9_branch01\")\n",
    "# model_3_pointer = model_3_branch01\n",
    "print(\"Model output 0 shape:\", model_3_branch01.shape) # 14 x 14 x (3 * (5 + classificationClass))\n",
    "model_3_branch01 = IdentityFinalLayer(name=\"model_3_outputLayer_0\")(model_3_branch01)\n",
    "print() # OUTPUT = model_3_branch01 (note: 13 x 13 grid untuk deteksi objek besar)\n",
    "\n",
    "model_3_actual = Model(inputs=model_3_input, outputs=[model_3_branch01, model_3_mergedBranch]) # mengikuti model dari https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "\n",
    "with open(\"./saved_models/model_3_inferenceModel.json\", \"wt\") as jsonFile:\n",
    "    jsonFile.write(model_3_actual.to_json())\n",
    "\n",
    "try:\n",
    "#     model_3_actual.load_weights(\"../keras-yolo3/model_data/yolov3_tiny_roundOverflowQ3_4.h5\", by_name=True, skip_mismatch=True)\n",
    "    model_3_actual.load_weights(\"./saved_models/model_3_checkpoint.h5\", by_name=True, skip_mismatch=True)\n",
    "    print(\"Weight load attempt success for model_3\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load existing model for model_3:\", e)\n",
    "try:\n",
    "    model_3_actual.save_weights(\"./saved_models/model_3_trainModel.h5\")\n",
    "    print(\"Loaded model is successfully re-saved\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save loaded model:\", e)\n",
    "# added code from https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "setArgs_model_3 = {\n",
    "    'anchors': tinyYolo_anchors, \n",
    "    'num_classes': classificationClass, \n",
    "    'ignore_thresh': 0.2\n",
    "}\n",
    "\n",
    "y_true_model_3 = [Input(shape=(image_height//{0:32, 1:16}[l], image_width//{0:32, 1:16}[l], 3, classificationClass + 5)) for l in range(2)]\n",
    "\n",
    "model_3_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss_model_3', arguments=setArgs_model_3)([*model_3_actual.output, *y_true_model_3])\n",
    "\n",
    "model_3 = Model([model_3_actual.input, *y_true_model_3], model_3_loss)\n",
    "\n",
    "# model_3_actual adalah working model yang akan dipakai untuk inference\n",
    "# y_true adalah layer input yang akan diisikan nilai sebenarnya\n",
    "# cara training model_3 adalah pendekatan sehingga output model_3 sedekat mungkin dengan 0 (model_3 ≈ model_3_actual - y_true)\n",
    "\n",
    "model_3.compile(optimizer=Adam(lr=1e-2), loss={'yolo_loss_model_3': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# loss adalah seberapa jauh nilai perbedaan output dengan 0\n",
    "\n",
    "print(\"Model model_3 compilation complete\") # tinggal porting line 54 sampai 61 dari https://github.com/awe777/keras-yolo3/blob/master/train.py, dilakukan di cell training di bawah\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_4 approximates Q3.4 signed fixed point operations with integer rules (overflow = positive -> negative & vice versa) \n",
    "# Done by rounding to the nearest 1/16 and capping at [-8, 8) after batch normalization and activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, None, None, 3)\n",
      "Branch split from main branch - following branch 0\n",
      "Branch split from branch 0 - following branch 0,0\n",
      "Branch merge from branch 1 and branch 0,0\n",
      "Model output 1 shape: (None, None, None, 255)\n",
      "\n",
      "Branch split from branch 0 - following branch 0,1\n",
      "Model output 0 shape: (None, None, None, 255)\n",
      "\n",
      "Weight load attempt success for model_4\n",
      "Loaded model is successfully re-saved\n",
      "Model model_4 compilation complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4 = None\n",
    "# '''\n",
    "model_4_input = Input(shape=(None, None, 3), name=\"model_4_inputLayer\")\n",
    "# model_4_pointer = model_4_input\n",
    "print(\"Input shape:\", model_4_input.shape) # 448 x 448 x 3\n",
    "model_4_startBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_input, layerFilter=16, name=\"model_4_layer0_branch\") \n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 448 x 448 x 16\n",
    "model_4_startBranch = MaxPooling2D(pool_size=(2, 2))(model_4_startBranch)\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 224 x 224 x 16\n",
    "model_4_startBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_startBranch, layerFilter=32, name=\"model_4_layer1_branch\")\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 224 x 224 x 32\n",
    "model_4_startBranch = MaxPooling2D(pool_size=(2, 2))(model_4_startBranch)\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 112 x 112 x 32\n",
    "model_4_startBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_startBranch, layerFilter=64, name=\"model_4_layer2_branch\")\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 112 x 112 x 64\n",
    "model_4_startBranch = MaxPooling2D(pool_size=(2, 2))(model_4_startBranch)\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 56 x 56 x 64\n",
    "model_4_startBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_startBranch, layerFilter=128, name=\"model_4_layer3_branch\")\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 56 x 56 x 128\n",
    "model_4_startBranch = MaxPooling2D(pool_size=(2, 2))(model_4_startBranch)\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 28 x 28 x 128\n",
    "model_4_startBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_startBranch, layerFilter=256, name=\"model_4_layer4_branch\")\n",
    "# model_4_pointer = model_4_startBranch\n",
    "# print(model_4_pointer.shape) # 28 x 28 x 256\n",
    "print(\"Branch split from main branch - following branch 0\") # 2 branch split from startBranch (28 x 28 x 256), following model_4_branch0\n",
    "model_4_branch0 = MaxPooling2D(pool_size=(2, 2))(model_4_startBranch)\n",
    "# model_4_pointer = model_4_branch0\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 256\n",
    "model_4_branch0 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch0, layerFilter=512, name=\"model_4_layer5_branch0\")\n",
    "# model_4_pointer = model_4_branch0\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 512\n",
    "model_4_branch0 = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(model_4_branch0)\n",
    "# model_4_pointer = model_4_branch0\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 512\n",
    "model_4_branch0 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch0, layerFilter=1024, name=\"model_4_layer6_branch0\")\n",
    "# model_4_pointer = model_4_branch0\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 1024\n",
    "model_4_branch0 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch0, layerFilter=256, kernelSize=(1, 1), name=\"model_4_layer7_branch0\")\n",
    "# model_4_pointer = model_4_branch0\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 256\n",
    "print(\"Branch split from branch 0 - following branch 0,0\") # 2 branch split from model_4_branch0 (14 x 14 x 256), following model_4_branch00\n",
    "model_4_branch00 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch0, layerFilter=128, kernelSize=(1, 1), name=\"model_4_layer8_branch00\")\n",
    "# model_4_pointer = model_4_branch00\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 128\n",
    "model_4_branch00 = UpSampling2D()(model_4_branch00)\n",
    "# model_4_pointer = model_4_branch00\n",
    "# print(model_4_pointer.shape) # 28 x 28 x 128\n",
    "print(\"Branch merge from branch 1 and branch 0,0\") # 2 branch merge from model_4_branch1 (unchanged from model_4_startBranch) and model_4_branch00\n",
    "model_4_mergedBranch = Concatenate()([model_4_startBranch, model_4_branch00])\n",
    "# model_4_pointer = model_4_mergedBranch\n",
    "# print(model_4_pointer.shape) # 28 x 28 x 384\n",
    "model_4_mergedBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_mergedBranch, layerFilter=256, name=\"model_4_layer9_branch1\")\n",
    "# model_4_pointer = model_4_mergedBranch\n",
    "# print(model_4_pointer.shape) # 28 x 28 x 256\n",
    "model_4_mergedBranch = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_mergedBranch, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_4_layerA_branch1\")\n",
    "# model_4_pointer = model_4_mergedBranch\n",
    "print(\"Model output 1 shape:\", model_4_mergedBranch.shape) # 28 x 28 x (3 * (5 + classificationClass))\n",
    "model_4_mergedBranch = IdentityFinalLayer(name=\"model_4_outputLayer_1\")(model_4_mergedBranch)\n",
    "print() # OUTPUT = model_4_mergedBranch (note: 26 x 26 grid untuk deteksi objek kecil)\n",
    "\n",
    "print(\"Branch split from branch 0 - following branch 0,1\")# following model_4_branch01\n",
    "model_4_branch01 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch0, layerFilter=512, name=\"model_4_layer8_branch01\")\n",
    "# model_4_pointer = model_4_branch01\n",
    "# print(model_4_pointer.shape) # 14 x 14 x 512\n",
    "model_4_branch01 = DBL(roundingFunction=RoundOverflowQ3_4, previousLayer=model_4_branch01, layerFilter=3 * (4 + 1 + classificationClass), name=\"model_4_layer9_branch01\")\n",
    "# model_4_pointer = model_4_branch01\n",
    "print(\"Model output 0 shape:\", model_4_branch01.shape) # 14 x 14 x (3 * (5 + classificationClass))\n",
    "model_4_branch01 = IdentityFinalLayer(name=\"model_4_outputLayer_0\")(model_4_branch01)\n",
    "print() # OUTPUT = model_4_branch01 (note: 13 x 13 grid untuk deteksi objek besar)\n",
    "\n",
    "model_4_actual = Model(inputs=model_4_input, outputs=[model_4_branch01, model_4_mergedBranch]) # mengikuti model dari https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "\n",
    "with open(\"./saved_models/model_4_inferenceModel.json\", \"wt\") as jsonFile:\n",
    "    jsonFile.write(model_4_actual.to_json())\n",
    "\n",
    "try:\n",
    "#     model_4_actual.load_weights(\"../keras-yolo3/model_data/yolov3_tiny_roundOverflowQ3_4.h5\", by_name=True, skip_mismatch=True)\n",
    "    model_4_actual.load_weights(\"./saved_models/model_4_checkpoint.h5\", by_name=True, skip_mismatch=True)\n",
    "    print(\"Weight load attempt success for model_4\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load existing model for model_4:\", e)\n",
    "try:\n",
    "    model_4_actual.save_weights(\"./saved_models/model_4_trainModel.h5\")\n",
    "    print(\"Loaded model is successfully re-saved\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save loaded model:\", e)\n",
    "# added code from https://github.com/awe777/keras-yolo3/blob/master/yolo3/model.py\n",
    "setArgs_model_4 = {\n",
    "    'anchors': tinyYolo_anchors, \n",
    "    'num_classes': classificationClass, \n",
    "    'ignore_thresh': 0.2\n",
    "}\n",
    "\n",
    "y_true_model_4 = [Input(shape=(image_height//{0:32, 1:16}[l], image_width//{0:32, 1:16}[l], 3, classificationClass + 5)) for l in range(2)]\n",
    "\n",
    "model_4_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss_model_4', arguments=setArgs_model_4)([*model_4_actual.output, *y_true_model_4])\n",
    "\n",
    "model_4 = Model([model_4_actual.input, *y_true_model_4], model_4_loss)\n",
    "\n",
    "# model_4_actual adalah working model yang akan dipakai untuk inference\n",
    "# y_true adalah layer input yang akan diisikan nilai sebenarnya\n",
    "# cara training model_4 adalah pendekatan sehingga output model_4 sedekat mungkin dengan 0 (model_4 ≈ model_4_actual - y_true)\n",
    "\n",
    "model_4.compile(optimizer=Adam(lr=1e-3), loss={'yolo_loss_model_4': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# loss adalah seberapa jauh nilai perbedaan output dengan 0\n",
    "\n",
    "print(\"Model model_4 compilation complete\") # tinggal porting line 54 sampai 61 dari https://github.com/awe777/keras-yolo3/blob/master/train.py, dilakukan di cell training di bawah\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_0 is not None:\n",
    "    with open(\"./saved_models/model_0_summary.txt\", \"wt\") as textFile:\n",
    "        model_0.summary(print_fn=lambda x: textFile.write(x + '\\n'), line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_1 is not None:\n",
    "    with open(\"./saved_models/model_1_summary.txt\", \"wt\") as textFile:\n",
    "        model_1.summary(print_fn=lambda x: textFile.write(x + '\\n'), line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_2 is not None:\n",
    "    with open(\"./saved_models/model_2_summary.txt\", \"wt\") as textFile:\n",
    "        model_2.summary(print_fn=lambda x: textFile.write(x + '\\n'), line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_3 is not None:\n",
    "    with open(\"./saved_models/model_3_summary.txt\", \"wt\") as textFile:\n",
    "        model_3.summary(print_fn=lambda x: textFile.write(x + '\\n'), line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_4 is not None:\n",
    "    with open(\"./saved_models/model_4_summary.txt\", \"wt\") as textFile:\n",
    "        model_4.summary(print_fn=lambda x: textFile.write(x + '\\n'), line_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "trainingBatchSize = 1\n",
    "epochSplit = 499 # because high split = more work saved; 117266 mod 499 = 1 <= less image lost\n",
    "train_data_generator = data_generator_wrapper(\n",
    "    annotation_lines=train_annotation_lines, \n",
    "    batch_size=trainingBatchSize, \n",
    "    input_shape=image_size, \n",
    "    anchors=tinyYolo_anchors, \n",
    "    num_classes=classificationClass\n",
    ")\n",
    "val_data_generator = data_generator_wrapper(\n",
    "    annotation_lines=val_annotation_lines, \n",
    "    batch_size=trainingBatchSize, \n",
    "    input_shape=image_size, \n",
    "    anchors=tinyYolo_anchors, \n",
    "    num_classes=classificationClass\n",
    ")\n",
    "\n",
    "# class ReloadOnNaN(Callback):\n",
    "#     def __init__(self, filepath=None):\n",
    "#         super(ReloadOnNaN, self).__init__()\n",
    "#         self.filepath = filepath\n",
    "#     def on_batch_end(self, batch, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         loss = logs.get('loss')\n",
    "#         if loss is not None:\n",
    "#             if np.isnan(loss) or np.isinf(loss):\n",
    "#                 if np.isnan(loss):\n",
    "#                     print('\\nDetected nan loss at batch %d, terminating training' % (batch))\n",
    "#                 else:\n",
    "#                     print('\\nDetected inf loss at batch %d, terminating training' % (batch))\n",
    "# #                 self.model.load_weights(self.filepath, by_name=True, skip_mismatch=True)\n",
    "# #                 self.model.reset_metrics()\n",
    "#                 self.model.stop_training = True\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "''' // comment on this line to enable/disable this block\n",
    "model_0_checkpoint_val = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_0_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_0_checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_0_checkpoint.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=2\n",
    ")\n",
    "model_0_learnRate = 1e-3\n",
    "model_0_LRDecay = math.pow(1 / 4, 1 / 3) # exponentially decays to 25% in 3 super-epochs \n",
    "superEpochs = 0\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 6 * 3600: # guarantees at least 6 hours of training\n",
    "# while superEpochs <= 10: # guarantees 10 super-epochs\n",
    "    superEpochs = superEpochs + 1\n",
    "    print('Time', str(time.time() - start_time))\n",
    "    print('Super-epoch', superEpochs)\n",
    "    print()\n",
    "    model_0_history = model_0.fit_generator(\n",
    "        generator=train_data_generator, \n",
    "        steps_per_epoch=max(1, (lenTrain // trainingBatchSize) // epochSplit), \n",
    "        epochs=1 * epochSplit,\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=max(1, 5 * (lenVal // trainingBatchSize) // epochSplit),\n",
    "        callbacks=[ \n",
    "            TerminateOnNaN(),\n",
    "            model_0_checkpoint_val,\n",
    "            model_0_checkpoint_loss\n",
    "        ]\n",
    "    )\n",
    "    model_0.save_weights(\"./saved_models/model_0_trainModel.h5\")\n",
    "    model_0_learnRate = model_0_LRDecay * model_0_learnRate\n",
    "    model_0.load_weights(\"./saved_models/model_0_checkpoint.h5\")\n",
    "    model_0.compile(optimizer=Adam(lr=model_0_learnRate), loss={'yolo_loss_model_0': lambda y_true, y_pred: y_pred})\n",
    "    print()\n",
    "print(\"model_0 training done in\", str(time.time() - start_time))\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "''' // comment on this line to enable/disable this block\n",
    "model_1_checkpoint_val = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_1_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_1_checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_1_checkpoint.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=2\n",
    ")\n",
    "model_1_learnRate = 1e-3\n",
    "model_1_LRDecay = math.pow(1 / 4, 1 / 3) # exponentially decays to 25% in 3 super-epochs \n",
    "superEpochs = 0\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 6 * 3600: # guarantees at least 6 hours of training\n",
    "# while superEpochs <= 10: # guarantees 10 super-epochs\n",
    "    superEpochs = superEpochs + 1\n",
    "    print('Time', str(time.time() - start_time))\n",
    "    print('Super-epoch', superEpochs)\n",
    "    print()\n",
    "    model_1_history = model_1.fit_generator(\n",
    "        generator=train_data_generator, \n",
    "        steps_per_epoch=max(1, (lenTrain // trainingBatchSize) // epochSplit), \n",
    "        epochs=1 * epochSplit,\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=max(1, 5 * (lenVal // trainingBatchSize) // epochSplit),\n",
    "        callbacks=[ \n",
    "            TerminateOnNaN(),\n",
    "            model_1_checkpoint_val,\n",
    "            model_1_checkpoint_loss\n",
    "        ]\n",
    "    )\n",
    "    model_1.save_weights(\"./saved_models/model_1_trainModel.h5\")\n",
    "    model_1_learnRate = model_1_LRDecay * model_1_learnRate\n",
    "    model_1.load_weights(\"./saved_models/model_1_checkpoint.h5\")\n",
    "    model_1.compile(optimizer=Adam(lr=model_1_learnRate), loss={'yolo_loss_model_1': lambda y_true, y_pred: y_pred})\n",
    "    print()\n",
    "print(\"model_1 training done in\", str(time.time() - start_time))\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "''' // comment on this line to enable/disable this block\n",
    "model_2_checkpoint_val = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_2_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_2_checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_2_checkpoint.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=2\n",
    ")\n",
    "model_2_learnRate = 1e-3\n",
    "model_2_LRDecay = math.pow(1 / 4, 1 / 3) # exponentially decays to 25% in 3 super-epochs \n",
    "superEpochs = 0\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 6 * 3600: # guarantees at least 6 hours of training\n",
    "# while superEpochs <= 10: # guarantees 10 super-epochs\n",
    "    superEpochs = superEpochs + 1\n",
    "    print('Time', str(time.time() - start_time))\n",
    "    print('Super-epoch', superEpochs)\n",
    "    print()\n",
    "    model_2_history = model_2.fit_generator(\n",
    "        generator=train_data_generator, \n",
    "        steps_per_epoch=max(1, (lenTrain // trainingBatchSize) // epochSplit), \n",
    "        epochs=1 * epochSplit,\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=max(1, 5 * (lenVal // trainingBatchSize) // epochSplit),\n",
    "        callbacks=[ \n",
    "            TerminateOnNaN(),\n",
    "            model_2_checkpoint_val,\n",
    "            model_2_checkpoint_loss\n",
    "        ]\n",
    "    )\n",
    "    model_2.save_weights(\"./saved_models/model_2_trainModel.h5\")\n",
    "    model_2_learnRate = model_2_LRDecay * model_2_learnRate\n",
    "    model_2.load_weights(\"./saved_models/model_2_checkpoint.h5\")\n",
    "    model_2.compile(optimizer=Adam(lr=model_2_learnRate), loss={'yolo_loss_model_2': lambda y_true, y_pred: y_pred})\n",
    "    print()\n",
    "print(\"model_2 training done in\", str(time.time() - start_time))\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "''' // comment on this line to enable/disable this block\n",
    "model_3_checkpoint_val = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_3_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_3_checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_3_checkpoint.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=2\n",
    ")\n",
    "model_3_learnRate = 1e-3\n",
    "model_3_LRDecay = math.pow(1 / 4, 1 / 3) # exponentially decays to 25% in 3 super-epochs \n",
    "superEpochs = 0\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 6 * 3600: # guarantees at least 6 hours of training\n",
    "# while superEpochs <= 10: # guarantees 10 super-epochs\n",
    "    superEpochs = superEpochs + 1\n",
    "    print('Time', str(time.time() - start_time))    \n",
    "    print('Super-epoch', superEpochs)\n",
    "    print()\n",
    "    model_3_history = model_3.fit_generator(\n",
    "        generator=train_data_generator, \n",
    "        steps_per_epoch=max(1, (lenTrain // trainingBatchSize) // epochSplit), \n",
    "        epochs=1 * epochSplit,\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=max(1, 5 * (lenVal // trainingBatchSize) // epochSplit),\n",
    "        callbacks=[ \n",
    "            TerminateOnNaN(),\n",
    "            model_3_checkpoint_val,\n",
    "            model_3_checkpoint_loss\n",
    "        ]\n",
    "    )\n",
    "    model_3.save_weights(\"./saved_models/model_3_trainModel.h5\")\n",
    "    model_3_learnRate = model_3_LRDecay * model_3_learnRate\n",
    "    model_3.load_weights(\"./saved_models/model_3_checkpoint.h5\")\n",
    "    model_3.compile(optimizer=Adam(lr=model_3_learnRate), loss={'yolo_loss_model_3': lambda y_true, y_pred: y_pred})\n",
    "    print()\n",
    "print(\"model_3 training done in\", str(time.time() - start_time))\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 0.0\n",
      "Super-epoch 1\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 81s 345ms/step - loss: 1864.6504 - val_loss: 2042.2230\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2042.22302, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1893.9753 - val_loss: 1526.2228\n",
      "\n",
      "Epoch 00002: val_loss improved from 2042.22302 to 1526.22278, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00002: loss improved from inf to 1893.97530, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1854.4593 - val_loss: 2005.7882\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1526.22278\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1838.4970 - val_loss: 1665.7560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1526.22278\n",
      "\n",
      "Epoch 00004: loss improved from 1893.97530 to 1838.49703, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1823.3219 - val_loss: 2146.7930\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1526.22278\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1905.4232 - val_loss: 1963.5776\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1526.22278\n",
      "\n",
      "Epoch 00006: loss did not improve from 1838.49703\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1830.2528 - val_loss: 1722.1599\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1526.22278\n",
      "Epoch 8/499\n",
      " 55/235 [======>.......................] - ETA: 43s - loss: nan      Batch 54: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1526.22278\n",
      "\n",
      "Epoch 00008: loss did not improve from 1838.49703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 505.73444652557373\n",
      "Super-epoch 2\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 306ms/step - loss: 1853.2431 - val_loss: 2800.9634\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1526.22278\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1865.9701 - val_loss: 1944.5674\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1526.22278\n",
      "\n",
      "Epoch 00002: loss did not improve from 1838.49703\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1822.6514 - val_loss: 1859.0348\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1526.22278\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1808.1588 - val_loss: 1657.1584\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1526.22278\n",
      "\n",
      "Epoch 00004: loss improved from 1838.49703 to 1808.15880, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 66s 283ms/step - loss: 1850.2851 - val_loss: 1513.0251\n",
      "\n",
      "Epoch 00005: val_loss improved from 1526.22278 to 1513.02515, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 67s 285ms/step - loss: 1859.5875 - val_loss: 1801.2245\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00006: loss did not improve from 1808.15880\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 66s 283ms/step - loss: 1864.6674 - val_loss: 2595.5664\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1513.02515\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1823.8540 - val_loss: 1577.8099\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00008: loss did not improve from 1808.15880\n",
      "Epoch 9/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1889.4407 - val_loss: 1709.7231\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1513.02515\n",
      "Epoch 10/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1808.8045 - val_loss: 1628.5444\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00010: loss did not improve from 1808.15880\n",
      "Epoch 11/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1844.3759 - val_loss: 1630.0801\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1513.02515\n",
      "Epoch 12/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1833.4038 - val_loss: 1641.9659\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00012: loss did not improve from 1808.15880\n",
      "Epoch 13/499\n",
      "235/235 [==============================] - 66s 282ms/step - loss: 1845.9662 - val_loss: 1690.2631\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1513.02515\n",
      "Epoch 14/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1847.5222 - val_loss: 1556.8645\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00014: loss did not improve from 1808.15880\n",
      "Epoch 15/499\n",
      "235/235 [==============================] - 68s 290ms/step - loss: 1821.6703 - val_loss: 2073.2644\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1513.02515\n",
      "Epoch 16/499\n",
      "235/235 [==============================] - 66s 283ms/step - loss: 1802.1679 - val_loss: 1718.2233\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00016: loss improved from 1808.15880 to 1802.16794, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 17/499\n",
      "235/235 [==============================] - 66s 282ms/step - loss: 1811.6291 - val_loss: 1615.1129\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1513.02515\n",
      "Epoch 18/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1817.6075 - val_loss: 1941.9595\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1513.02515\n",
      "\n",
      "Epoch 00018: loss did not improve from 1802.16794\n",
      "Epoch 19/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1844.8037 - val_loss: 1755.5182\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1513.02515\n",
      "Epoch 20/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1810.4388 - val_loss: 1511.3397\n",
      "\n",
      "Epoch 00020: val_loss improved from 1513.02515 to 1511.33972, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00020: loss did not improve from 1802.16794\n",
      "Epoch 21/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1813.6639 - val_loss: 1499.6820\n",
      "\n",
      "Epoch 00021: val_loss improved from 1511.33972 to 1499.68201, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 22/499\n",
      "235/235 [==============================] - 66s 282ms/step - loss: 1888.4041 - val_loss: 1527.8644\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1499.68201\n",
      "\n",
      "Epoch 00022: loss did not improve from 1802.16794\n",
      "Epoch 23/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1814.3786 - val_loss: 2316.3083\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1499.68201\n",
      "Epoch 24/499\n",
      "235/235 [==============================] - 67s 285ms/step - loss: 1832.4344 - val_loss: 2048.3391\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1499.68201\n",
      "\n",
      "Epoch 00024: loss did not improve from 1802.16794\n",
      "Epoch 25/499\n",
      "235/235 [==============================] - 67s 283ms/step - loss: 1819.2646 - val_loss: 1606.4574\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1499.68201\n",
      "Epoch 26/499\n",
      "235/235 [==============================] - 67s 283ms/step - loss: 1797.7986 - val_loss: 1574.4020\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1499.68201\n",
      "\n",
      "Epoch 00026: loss improved from 1802.16794 to 1797.79862, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 27/499\n",
      " 80/235 [=========>....................] - ETA: 38s - loss: nan      Batch 79: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1499.68201\n",
      "\n",
      "Time 2277.7665462493896\n",
      "Super-epoch 3\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 305ms/step - loss: 1824.2605 - val_loss: 2001.9299\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1499.68201\n",
      "\n",
      "Epoch 00001: loss did not improve from 1797.79862\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1786.0442 - val_loss: 2295.3169\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1499.68201\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1808.1452 - val_loss: 2114.8381\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1499.68201\n",
      "\n",
      "Epoch 00003: loss did not improve from 1797.79862\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1800.8031 - val_loss: 2268.1650\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1499.68201\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1802.2491 - val_loss: 1484.4589\n",
      "\n",
      "Epoch 00005: val_loss improved from 1499.68201 to 1484.45886, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00005: loss did not improve from 1797.79862\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1798.4794 - val_loss: 2856.4556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1484.45886\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1775.0374 - val_loss: 1612.6482\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1484.45886\n",
      "\n",
      "Epoch 00007: loss improved from 1797.79862 to 1775.03737, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1823.1382 - val_loss: 1584.2782\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1484.45886\n",
      "Epoch 9/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1824.6852 - val_loss: 1766.5430\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1484.45886\n",
      "\n",
      "Epoch 00009: loss did not improve from 1775.03737\n",
      "Epoch 10/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1776.9154 - val_loss: 2370.7996\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1484.45886\n",
      "Epoch 11/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1778.0624 - val_loss: 1590.0289\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1484.45886\n",
      "\n",
      "Epoch 00011: loss did not improve from 1775.03737\n",
      "Epoch 12/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1792.5795 - val_loss: 1526.3839\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1484.45886\n",
      "Epoch 13/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1806.4833 - val_loss: 2892.7954\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1484.45886\n",
      "\n",
      "Epoch 00013: loss did not improve from 1775.03737\n",
      "Epoch 14/499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 65s 277ms/step - loss: 1791.8774 - val_loss: 2421.3164\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1484.45886\n",
      "Epoch 15/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1784.9380 - val_loss: 1940.3721\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1484.45886\n",
      "\n",
      "Epoch 00015: loss did not improve from 1775.03737\n",
      "Epoch 16/499\n",
      "221/235 [===========================>..] - ETA: 3s - loss: nan      Batch 220: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1484.45886\n",
      "\n",
      "Time 3328.736638069153\n",
      "Super-epoch 4\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 299ms/step - loss: 1801.1086 - val_loss: 1469.5974\n",
      "\n",
      "Epoch 00001: val_loss improved from 1484.45886 to 1469.59741, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00001: loss did not improve from 1775.03737\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 64s 274ms/step - loss: 1826.2356 - val_loss: 1603.3123\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 64s 274ms/step - loss: 1809.2571 - val_loss: 1944.5436\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00003: loss did not improve from 1775.03737\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 64s 274ms/step - loss: 1784.5933 - val_loss: 1725.5177\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1469.59741\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 64s 273ms/step - loss: 1792.9923 - val_loss: 1621.6199\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00005: loss did not improve from 1775.03737\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 64s 274ms/step - loss: 1775.3822 - val_loss: 1913.1067\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1469.59741\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 65s 274ms/step - loss: 1770.0189 - val_loss: 1505.4019\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00007: loss improved from 1775.03737 to 1770.01890, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 64s 273ms/step - loss: 1826.3248 - val_loss: 1910.0048\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1469.59741\n",
      "Epoch 9/499\n",
      " 34/235 [===>..........................] - ETA: 48s - loss: nan      Batch 33: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00009: loss did not improve from 1770.01890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 3874.918766975403\n",
      "Super-epoch 5\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 297ms/step - loss: 1800.6756 - val_loss: 1850.6481\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 1789.6913 - val_loss: 2295.3396\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00002: loss did not improve from 1770.01890\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1795.6129 - val_loss: 1646.6139\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1776.4336 - val_loss: 1535.0510\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00004: loss did not improve from 1770.01890\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1791.0869 - val_loss: 1704.1986\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1469.59741\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 1800.0825 - val_loss: 1634.6185\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00006: loss did not improve from 1770.01890\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 1755.9051 - val_loss: 1513.6443\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1469.59741\n",
      "Epoch 8/499\n",
      "157/235 [===================>..........] - ETA: 18s - loss: nan      Batch 156: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00008: loss did not improve from 1770.01890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 4386.761391162872\n",
      "Super-epoch 6\n",
      "\n",
      "Epoch 1/499\n",
      "219/235 [==========================>...] - ETA: 4s - loss: nan      Batch 218: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 4460.576580762863\n",
      "Super-epoch 7\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 298ms/step - loss: 1769.3765 - val_loss: 1572.8330\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00001: loss improved from 1770.01890 to 1769.37652, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1772.1666 - val_loss: 1894.2637\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 1791.2154 - val_loss: 1768.3704\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n",
      "Epoch 4/499\n",
      "  6/235 [..............................] - ETA: 54s - loss: nan      Batch 5: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1469.59741\n",
      "\n",
      "Time 4679.515100717545\n",
      "Super-epoch 8\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 300ms/step - loss: 1817.8996 - val_loss: 1555.8849\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1752.2008 - val_loss: 1639.8560\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "Epoch 3/499\n",
      " 41/235 [====>.........................] - ETA: 46s - loss: nan      Batch 40: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 4843.348004102707\n",
      "Super-epoch 9\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 300ms/step - loss: 1811.6455 - val_loss: 1631.9342\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1779.2975 - val_loss: 1573.6995\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1752.4580 - val_loss: 1589.2689\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1791.9626 - val_loss: 2377.7485\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1787.7884 - val_loss: 1571.8354\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1469.59741\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1860.3222 - val_loss: 1542.5789\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00006: loss did not improve from 1769.37652\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1773.3442 - val_loss: 1623.7666\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1469.59741\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1778.9702 - val_loss: 1640.3201\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00008: loss did not improve from 1769.37652\n",
      "Epoch 9/499\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 1825.1851 - val_loss: 1728.4528\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1469.59741\n",
      "Epoch 10/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1811.8734 - val_loss: 1579.9163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00010: loss did not improve from 1769.37652\n",
      "Epoch 11/499\n",
      "109/235 [============>.................] - ETA: 30s - loss: nan      Batch 108: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1469.59741\n",
      "\n",
      "Time 5544.961642742157\n",
      "Super-epoch 10\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 303ms/step - loss: 1827.3119 - val_loss: 2470.7146\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1792.5123 - val_loss: 1478.2662\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1469.59741\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1772.3500 - val_loss: 2120.0935\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 65s 279ms/step - loss: 1778.3564 - val_loss: 2467.8818\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1469.59741\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1782.9837 - val_loss: 1596.1387\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00005: loss did not improve from 1769.37652\n",
      "Epoch 6/499\n",
      "  9/235 [>.............................] - ETA: 55s - loss: nan      Batch 8: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1469.59741\n",
      "\n",
      "Time 5902.541484832764\n",
      "Super-epoch 11\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 70s 299ms/step - loss: 1806.1192 - val_loss: 1514.8226\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1469.59741\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 1799.7465 - val_loss: 1466.4048\n",
      "\n",
      "Epoch 00002: val_loss improved from 1469.59741 to 1466.40479, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 3/499\n",
      " 24/235 [==>...........................] - ETA: 51s - loss: nan      Batch 23: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1466.40479\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 6068.155118942261\n",
      "Super-epoch 12\n",
      "\n",
      "Epoch 1/499\n",
      " 65/235 [=======>......................] - ETA: 53s - loss: nan      Batch 64: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 6113.627331733704\n",
      "Super-epoch 13\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 301ms/step - loss: 1795.4624 - val_loss: 3114.0708\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1466.40479\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      " 69/235 [=======>......................] - ETA: 40s - loss: nan      Batch 68: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1466.40479\n",
      "\n",
      "Time 6228.184859752655\n",
      "Super-epoch 14\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 301ms/step - loss: 1815.0569 - val_loss: 1555.5898\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1466.40479\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      " 96/235 [===========>..................] - ETA: 33s - loss: nan      Batch 95: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1466.40479\n",
      "\n",
      "Time 6351.694731712341\n",
      "Super-epoch 15\n",
      "\n",
      "Epoch 1/499\n",
      "188/235 [=======================>......] - ETA: 12s - loss: nan      Batch 187: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 6434.392139911652\n",
      "Super-epoch 16\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 303ms/step - loss: 1801.0356 - val_loss: 1601.3888\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1466.40479\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 278ms/step - loss: 1801.8066 - val_loss: 1580.6765\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1466.40479\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 65s 278ms/step - loss: 1806.7867 - val_loss: 2855.9529\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1466.40479\n",
      "Epoch 4/499\n",
      "234/235 [============================>.] - ETA: 0s - loss: nan      Batch 233: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1466.40479\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 6727.427360773087\n",
      "Super-epoch 17\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 301ms/step - loss: 1765.6701 - val_loss: 2462.6580\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1466.40479\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 278ms/step - loss: 1814.1227 - val_loss: 1449.9647\n",
      "\n",
      "Epoch 00002: val_loss improved from 1466.40479 to 1449.96472, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1785.5643 - val_loss: 1470.2100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1449.96472\n",
      "Epoch 4/499\n",
      "232/235 [============================>.] - ETA: 0s - loss: nan      Batch 231: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 7023.1567170619965\n",
      "Super-epoch 18\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 305ms/step - loss: 1787.6895 - val_loss: 1450.9281\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1449.96472\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1816.6832 - val_loss: 2110.3264\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1801.4047 - val_loss: 1571.1893\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1449.96472\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1786.6112 - val_loss: 2696.7117\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n",
      "Epoch 5/499\n",
      "118/235 [==============>...............] - ETA: 28s - loss: nan      Batch 117: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1449.96472\n",
      "\n",
      "Time 7361.813494443893\n",
      "Super-epoch 19\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 307ms/step - loss: 1856.5453 - val_loss: 1961.3053\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1805.5242 - val_loss: 1453.1836\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1449.96472\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 1798.4505 - val_loss: 1492.1340\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1805.7832 - val_loss: 1707.8525\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1449.96472\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1800.3075 - val_loss: 1560.3958\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00005: loss did not improve from 1769.37652\n",
      "Epoch 6/499\n",
      "179/235 [=====================>........] - ETA: 13s - loss: nan      Batch 178: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1449.96472\n",
      "\n",
      "Time 7784.434248209\n",
      "Super-epoch 20\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 304ms/step - loss: 1794.1174 - val_loss: 1710.0986\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1799.4755 - val_loss: 1753.9390\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1449.96472\n",
      "Epoch 3/499\n",
      "218/235 [==========================>...] - ETA: 4s - loss: nan      Batch 217: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8023.083443641663\n",
      "Super-epoch 21\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 71s 304ms/step - loss: 1792.0486 - val_loss: 1886.7219\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1449.96472\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 278ms/step - loss: 1774.8303 - val_loss: 1462.1731\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1449.96472\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "152/235 [==================>...........] - ETA: 20s - loss: nan      Batch 151: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1449.96472\n",
      "\n",
      "Time 8245.74611401558\n",
      "Super-epoch 22\n",
      "\n",
      "Epoch 1/499\n",
      " 18/235 [=>............................] - ETA: 1:56 - loss: nan      Batch 17: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8309.377002239227\n",
      "Super-epoch 23\n",
      "\n",
      "Epoch 1/499\n",
      " 58/235 [======>.......................] - ETA: 59s - loss: nan       Batch 57: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8386.009662866592\n",
      "Super-epoch 24\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 306ms/step - loss: 1842.6754 - val_loss: 1424.6265\n",
      "\n",
      "Epoch 00001: val_loss improved from 1449.96472 to 1424.62646, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 65s 278ms/step - loss: 1783.2626 - val_loss: 1472.3899\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "Epoch 3/499\n",
      "149/235 [==================>...........] - ETA: 20s - loss: nan      Batch 148: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8621.265328645706\n",
      "Super-epoch 25\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 308ms/step - loss: 1813.6605 - val_loss: 1557.9569\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "Epoch 2/499\n",
      "129/235 [===============>..............] - ETA: 26s - loss: nan      Batch 128: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8791.963084220886\n",
      "Super-epoch 26\n",
      "\n",
      "Epoch 1/499\n",
      "168/235 [====================>.........] - ETA: 18s - loss: nan      Batch 167: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 8908.79643034935\n",
      "Super-epoch 27\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 307ms/step - loss: 1809.9655 - val_loss: 2209.2463\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1756.6188 - val_loss: 1526.5250\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 66s 279ms/step - loss: 1777.7081 - val_loss: 1452.8232\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00003: loss did not improve from 1769.37652\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 74s 313ms/step - loss: 1799.9719 - val_loss: 1663.1697\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1424.62646\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 68s 290ms/step - loss: 1843.4250 - val_loss: 1866.2100\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00005: loss did not improve from 1769.37652\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 67s 286ms/step - loss: 1779.5453 - val_loss: 1717.9229\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1424.62646\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 68s 290ms/step - loss: 1828.3072 - val_loss: 1520.5803\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00007: loss did not improve from 1769.37652\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1788.4535 - val_loss: 1853.7252\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1424.62646\n",
      "Epoch 9/499\n",
      "235/235 [==============================] - 66s 282ms/step - loss: 1802.3733 - val_loss: 1549.7726\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00009: loss did not improve from 1769.37652\n",
      "Epoch 10/499\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 1829.6758 - val_loss: 2527.3865\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1424.62646\n",
      "Epoch 11/499\n",
      " 20/235 [=>............................] - ETA: 53s - loss: nan      Batch 19: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00011: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 9668.070798873901\n",
      "Super-epoch 28\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 72s 308ms/step - loss: 1761.6382 - val_loss: 2408.0461\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 283ms/step - loss: 1798.2526 - val_loss: 1673.8861\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 67s 283ms/step - loss: 1786.6538 - val_loss: 2060.5405\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "Epoch 4/499\n",
      "202/235 [========================>.....] - ETA: 8s - loss: nan      Batch 201: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 10004.982074737549\n",
      "Super-epoch 29\n",
      "\n",
      "Epoch 1/499\n",
      " 82/235 [=========>....................] - ETA: 48s - loss: nan      Batch 81: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 10115.671771764755\n",
      "Super-epoch 30\n",
      "\n",
      "Epoch 1/499\n",
      "152/235 [==================>...........] - ETA: 23s - loss: nan      Batch 151: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 10247.714269638062\n",
      "Super-epoch 31\n",
      "\n",
      "Epoch 1/499\n",
      "221/235 [===========================>..] - ETA: 3s - loss: nan      Batch 220: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 10403.260318756104\n",
      "Super-epoch 32\n",
      "\n",
      "Epoch 1/499\n",
      " 88/235 [==========>...................] - ETA: 45s - loss: nan      Batch 87: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1769.37652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 10531.437085866928\n",
      "Super-epoch 33\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 73s 312ms/step - loss: 1809.3956 - val_loss: 1485.9390\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 67s 287ms/step - loss: 1813.9608 - val_loss: 1444.8381\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00002: loss did not improve from 1769.37652\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1834.4925 - val_loss: 2062.8320\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 67s 285ms/step - loss: 1794.4342 - val_loss: 1846.6353\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00004: loss did not improve from 1769.37652\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 67s 287ms/step - loss: 1790.7916 - val_loss: 2186.6165\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1424.62646\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 67s 285ms/step - loss: 1769.1233 - val_loss: 1525.8533\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00006: loss improved from 1769.37652 to 1769.12334, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 67s 285ms/step - loss: 1771.6285 - val_loss: 1718.2157\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1424.62646\n",
      "Epoch 8/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1760.6262 - val_loss: 1504.0759\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00008: loss improved from 1769.12334 to 1760.62624, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 9/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1830.4298 - val_loss: 1709.0942\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1424.62646\n",
      "Epoch 10/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1805.0096 - val_loss: 2411.7961\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00010: loss did not improve from 1760.62624\n",
      "Epoch 11/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1816.5699 - val_loss: 1523.9286\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1424.62646\n",
      "Epoch 12/499\n",
      "235/235 [==============================] - 67s 283ms/step - loss: 1771.8471 - val_loss: 2410.3376\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00012: loss did not improve from 1760.62624\n",
      "Epoch 13/499\n",
      "235/235 [==============================] - 70s 298ms/step - loss: 1764.6902 - val_loss: 1483.6309\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1424.62646\n",
      "Epoch 14/499\n",
      "235/235 [==============================] - 70s 296ms/step - loss: 1819.5194 - val_loss: 1748.8591\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00014: loss did not improve from 1760.62624\n",
      "Epoch 15/499\n",
      "235/235 [==============================] - 68s 291ms/step - loss: 1791.7499 - val_loss: 1611.5645\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1424.62646\n",
      "Epoch 16/499\n",
      "135/235 [================>.............] - ETA: 24s - loss: nan      Batch 134: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00016: loss did not improve from 1760.62624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 11692.653341531754\n",
      "Super-epoch 34\n",
      "\n",
      "Epoch 1/499\n",
      "229/235 [============================>.] - ETA: 1s - loss: nan      Batch 228: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 11864.242898464203\n",
      "Super-epoch 35\n",
      "\n",
      "Epoch 1/499\n",
      " 20/235 [=>............................] - ETA: 1:54 - loss: nan      Batch 19: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1760.62624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 11991.063078641891\n",
      "Super-epoch 36\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 73s 310ms/step - loss: 1796.0146 - val_loss: 1521.1064\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 68s 289ms/step - loss: 1756.8587 - val_loss: 1920.1465\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00002: loss improved from 1760.62624 to 1756.85865, saving model to ./saved_models/model_4_checkpoint.h5\n",
      "Epoch 3/499\n",
      "235/235 [==============================] - 67s 287ms/step - loss: 1801.2531 - val_loss: 2236.3665\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "Epoch 4/499\n",
      "235/235 [==============================] - 68s 288ms/step - loss: 1812.2832 - val_loss: 1684.3143\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00004: loss did not improve from 1756.85865\n",
      "Epoch 5/499\n",
      "235/235 [==============================] - 67s 284ms/step - loss: 1773.5102 - val_loss: 1892.4014\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1424.62646\n",
      "Epoch 6/499\n",
      "235/235 [==============================] - 67s 283ms/step - loss: 1758.1284 - val_loss: 1461.5490\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00006: loss did not improve from 1756.85865\n",
      "Epoch 7/499\n",
      "235/235 [==============================] - 66s 282ms/step - loss: 1797.2462 - val_loss: 1547.0798\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1424.62646\n",
      "Epoch 8/499\n",
      "177/235 [=====================>........] - ETA: 14s - loss: nan      Batch 176: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00008: loss did not improve from 1756.85865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 12634.634366512299\n",
      "Super-epoch 37\n",
      "\n",
      "Epoch 1/499\n",
      "235/235 [==============================] - 74s 314ms/step - loss: 1835.6424 - val_loss: 2277.0969\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1424.62646\n",
      "Epoch 2/499\n",
      "235/235 [==============================] - 66s 283ms/step - loss: 1784.9001 - val_loss: 1475.9468\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1424.62646\n",
      "\n",
      "Epoch 00002: loss did not improve from 1756.85865\n",
      "Epoch 3/499\n",
      " 64/235 [=======>......................] - ETA: 42s - loss: nan      Batch 63: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1424.62646\n",
      "\n",
      "Time 12919.004578113556\n",
      "Super-epoch 38\n",
      "\n",
      "Epoch 1/499\n",
      "139/235 [================>.............] - ETA: 29s - loss: nan      Batch 138: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00001: loss did not improve from 1756.85865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "b:\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:709: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time 13100.041555643082\n",
      "Super-epoch 39\n",
      "\n",
      "Epoch 1/499\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "# ''' // comment on this line to enable/disable this block\n",
    "model_4_checkpoint_val = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_4_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_4_checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='./saved_models/model_4_checkpoint.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    period=2\n",
    ")\n",
    "model_4_learnRate = 1e-3\n",
    "model_4_LRDecay = math.pow(1 / 4, 1 / 3) # exponentially decays to 25% in 3 super-epochs \n",
    "superEpochs = 0\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 6 * 3600: # guarantees at least 6 hours of training\n",
    "# while superEpochs <= 10: # guarantees 10 super-epochs\n",
    "    superEpochs = superEpochs + 1\n",
    "    print('Time', str(time.time() - start_time))\n",
    "    print('Super-epoch', superEpochs)\n",
    "    print()\n",
    "    model_4_history = model_4.fit_generator(\n",
    "        generator=train_data_generator, \n",
    "        steps_per_epoch=max(1, (lenTrain // trainingBatchSize) // epochSplit), \n",
    "        epochs=1 * epochSplit,\n",
    "        verbose=2,\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=max(1, 5 * (lenVal // trainingBatchSize) // epochSplit),\n",
    "        callbacks=[ \n",
    "            TerminateOnNaN(),\n",
    "            model_4_checkpoint_val,\n",
    "            model_4_checkpoint_loss\n",
    "        ]\n",
    "    )\n",
    "    model_4.save_weights(\"./saved_models/model_4_trainModel.h5\")\n",
    "    model_4_learnRate = model_4_LRDecay * model_4_learnRate\n",
    "    model_4.load_weights(\"./saved_models/model_4_checkpoint.h5\")\n",
    "    model_4.compile(optimizer=Adam(lr=model_4_learnRate), loss={'yolo_loss_model_4': lambda y_true, y_pred: y_pred})\n",
    "    print()\n",
    "print(\"model_4 training done in\", str(time.time() - start_time))\n",
    "# '''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resource successfully released\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
